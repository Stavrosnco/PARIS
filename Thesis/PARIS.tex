% Cal Poly Thesis
% 
% based on UC Thesis format
%
% modified by Mark Barry 2/07.
%

\documentclass[12pt]{ucthesis}
\usepackage{ifpdf}

\let\ifpdf\relax

\newif\ifpdf
\ifx\pdfoutput\undefined
    \pdffalse % we are not running PDFLaTeX
\else
\pdfoutput=1 % we are running PDFLaTeX
\pdftrue \fi

\usepackage{url}
\ifpdf

    \usepackage[pdftex]{graphicx}
    % Update title and author below...
    \usepackage[pdftex,plainpages=false,breaklinks=true,colorlinks=true,urlcolor=blue,citecolor=blue,%
                                       linkcolor=blue,bookmarks=true,bookmarksopen=true,%
                                       bookmarksopenlevel=3,pdfstartview=FitV,
                                       pdfauthor={Joseph White},
                                       pdftitle={PARIS: A PArallel RSA-prime InSpection tool},
                                       pdfkeywords={thesis, masters, cal poly}
                                       ]{hyperref}
    %Options with pdfstartview are FitV, FitB and FitH
    \pdfcompresslevel=1

\else
    \usepackage{graphicx}
\fi
\def\newblock{\hskip .11em plus .33em minus .07em} % important line
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[letterpaper]{geometry}
\usepackage[overload]{textcase}
\usepackage{url}
\usepackage{array}
\usepackage[ruled]{algorithm2e}
\usepackage{ifthen}

\setlength{\parindent}{0.25in} \setlength{\parskip}{6pt}

\geometry{verbose,nohead,tmargin=1.25in,bmargin=1in,lmargin=1.5in,rmargin=1.3in}

\setcounter{tocdepth}{2}

% Different font in captions (single-spaced, bold) ------------
\newcommand{\captionfonts}{\small\bf\ssp}
% ---------------------------------------

\begin{document}

\title{PARIS: A PArallel RSA-prime InSpection tool}
\author{Joseph White}
\degreemonth{June} \degreeyear{2013} \degree{Master of Science}
\defensemonth{May} \defenseyear{2013}
\numberofmembers{3} \chair{Chris Lupo, Ph.D.} \othermemberA{Phillip Nico, Ph.D.} \othermemberB{Foaad Khosmood, Ph.D.}
\field{Computer Science} \campus{San Luis Obispo}
\maketitle

\begin{frontmatter}
% Custom made for Cal Poly (by Mark Barry, modified by Andrew Tsui).
\copyrightpage
% Custom made for Cal Poly (by Andrew Tsui).
\committeemembershippage


\begin{abstract}

Modern-day computer security relies heavily on cryptography as a means to
protect the data that we have become increasingly reliant on. As the Internet
becomes more ubiquitous, methods of security must be better than ever.
Validation tools can be leveraged to help increase our confidence and
accountability for methods we employ to secure our systems. 

Security validation, however, can be difficult and time-consuming. As our
computational ability increases, calculations that were once considered ``hard"
due to length of computation, can now be done in minutes. We are constantly
increasing the size of our keys and attempting to make computations harder to
protect our information. This increase in ``cracking" difficulty often has the
unfortunate side-effect of making validation equally as difficult.

We can leverage massive-parallelism and the computational power that is granted
by today's commodity hardware such as GPUs to make checks that would
otherwise be impossible to perform, attainable. Our work presents a practical
tool for validating RSA keys for poor prime numbers: a fundamental problem
that has led to significant security holes.

Our implementation using NVIDIA's CUDA framework offers a 27.5 times speedup
over a reference sequential implementation. This level of speedup brings this
validation into the realm of runtime reachability.


\end{abstract}


\tableofcontents


\listoftables
\listoffigures
\end{frontmatter}


\pagestyle{plain}
\renewcommand{\baselinestretch}{1.66}

% ------------- Main chapters here --------------------
\begin{comment}
\chapter{Introduction}
\label{sec:intro}
RSA is a public key encryption scheme which relies on the difficulty of 
factoring large numbers. The algorithm is prevalent throughout security, 
specifically it is used for many web-security applications. An RSA public key is 
comprised of a modulus $n$ of specified length (the product of primes $p$ and 
$q$), and an exponent $e$. The length of $n$ is given in terms of bits, thus 
the term ``1024-bit RSA key" refers to the number of bits which make up this 
value. The associated private key uses the same $n$, and another value $d$ such 
that $d \cdot e = 1 \:\text{mod} \;\phi(n)$ where $\phi(n) = (p - 1) \cdot
(q - 1)$\cite{rsa}. Ideally, given the number of possible primes that may be
used to construct a 1024-bit modulus, no random number generators should reuse
either prime. Thus, the likelihood of either $p$ or $q$ being repeated in a set
of keys should be approximately 0. An individual key may be considered secure by
itself, but when compared to other keys, might exhibit a weakness which allows
each key's $d$ to be calculated entirely from public information.

When considering two keys, a weakness exists when the greatest common 
divisor of both moduli, $n_1$ and $n_2$, is greater than 1. If $GCD(n_1, 
n_2) = p$, then $p$ must be a shared prime factor of $n_1$ and $n_2$. Thus, 
$q_1 = \frac{n_1}{p}$ and $q_2 = \frac{n_2}{p}$. Once $p$ and $q$ are known, 
$d_1$ and $d_2$ can be directly calculated, yielding both private key pairs.
This vulnerability is detailed in \ref{fig:vuln}.

This weakness is discussed in \cite{lenstra2012ron}, which showed a 
significant number of existing RSA keys were susceptible to this exploit. The 
primary goal of our work was to speedup the most computationally intensive 
part of their process by implementing the GCD comparisons of RSA 1024-bit keys 
using NVIDIA's CUDA platform.

To aid in accomplishing this goal, the work in \cite{fujimoto2009high} was 
expanded and adapted to compare all combinations of keys in a given set. In 
comparison to their work, larger sections of the overall program were able to 
be executed in parallel, resulting in further speedup.

%
%\section{Related Work}
%The work documented in \cite{lenstra:ron} served as inspiration for this work.
%Here, Lenstra et al. performed a sanity check of a wide array of public RSA keys 
%contained in SSL certificates and SSH host keys. Their discovery that a 
%significant fraction of these keys (roughly 0.2\%) were weak led to our 
%desire to parallelize their investigation, in order to make it as efficient as 
%possible with commodity hardware. 
%
%The CUDA implementation of the binary GCD algorithm that was built upon 
%(cf. \cite{fujimoto2009high}) is an important example of similar work being 
%done. On a fundamental level, our work mirrors theirs as we based the core of 
%our algorithm on their work, specifically 1024-bit GCDs were calculated in 
%parallel using CUDA. However, we expanded its relevance with modifications in 
%order to automatically divide and parallelize lists of large values to compare. 
%
%Another example of work that makes use of the GPU for security applications is 
%solving discrete logarithms as presented in \cite{henrysolving}. 
%A set of large-precision operations (768-bit) was necessary for this work, and 
%was thus implemented in CUDA. This was similar to our own starting point due to 
%the currently-limited CUDA support for large-precision numbers. Because these 
%large values have numerous applications in computer security, the work shown 
%here displays another component of computer security where parallelizing work 
%with the large values can be highly advantageous. 
%
%Our work is an example of an amalgamation of other related works. It functions 
%as a supplement to the other materials mentioned here, and provides another 
%example of a computer security application that significantly benefits from 
%using parallelization with commodity Single Instruction, Multiple Data (SIMD) 
%multiprocessors. What sets it apart is its use of 1024-bit RSA keys and the 
%method of parallelization implemented. 
%

\section{Overview of CUDA}
\label{sec:cuda}
CUDA is a platform that provides a set of tools along with the ability to 
write programs that make use of NVIDIA's GPUs (cf. 
\cite{nvidia2012programming}). These massively-parallel hardware devices are 
capable of processing large amounts of data simultaneously, allowing 
significant speedups in programs with sections of parallelizable code using 
the SIMD model. The platform allows for various arrangements of threads to 
perform work, based on the developer's decomposition of the problem. Our 
solution to the problem presented in this paper is discussed in 
$\S$\ref{sec:GridOrg}. In general, individual threads are grouped into up-to 
3-dimensional blocks to allow sharing of common memory between threads. These 
blocks can then be organized into a 2-dimensional grid. 

The GPU breaks the total number of threads into groups called warps, which 
consist of 32 threads that will be executed simultaneously on a single
\textit{streaming multiprocessor} (SM). The GPU consists of several SMs which 
are each capable of executing a warp. Blocks are scheduled to SMs until all 
allocated threads have been executed. 

There is also a memory hierarchy on the GPU. There are 3 types of memory 
that are relevant to this work: global memory is the slowest and 
largest; shared memory is much faster, but also significantly smaller; and 
a limited number of registers that each SM has access to. Each thread in a
block can access the same section of shared memory.

\section{Algorithm Description}
\label{sec:alg}
\subsection{Binary GCD}
\label{subsec:binGCD}
Binary GCD is a well known algorithm for computing the greatest common divisor 
of two numbers. Instead of relying on costly division operations like Euclid's 
algorithm, bit-wise shifts are employed. The implementation
presented in this paper follows the outline displayed in Algorithm~\ref{alg:binGCD}.

\begin{algorithm}
   \label{alg:binGCD}
   \KwIn{$x$ and $y$: two integers.}
   \KwOut{The greatest common divisor of $x$ and $y$.}
   \nl\Repeat {$GCD(x, y) = GCD(0, y) = y$} {
      \nl\uIf {$x$ and $y$ are both even} {
         \nl$GCD(x, y) = 2 \cdot GCD(\frac{x}{2}, \frac{y}{2})$\;
      }\nl\uElseIf{$x$ is even and $y$ is odd} {
         \nl$GCD(x, y) = GCD(\frac{x}{2}, y)$\;
      }\nl\uElseIf {$x$ is odd and $y$ is even} {
         \nl$GCD(x, y) = GCD(\frac{y}{2}, x)$\;
      }\nl\ElseIf {$x$ and $y$ are both odd} {
         \nl\eIf {$x \geq y$} {
            \nl$GCD(x, y) = GCD(\frac{x - y}{2}, y)$\;
         }{
            \nl$GCD(x, y) = GCD(\frac{y - x}{2}, x)$\;
        }
      }
   }
   \caption{Binary GCD algorithm outline}
\end{algorithm}

\subsection{Parallel Functions}
\label{subsec:parfunc}
To accomplish Algorithm \ref{alg:binGCD} using CUDA, the following three 
functions had to parallelized: shift, subtract, and greater-than-or-equal. As 
outlined in \cite{fujimoto2009high}, each 1024-bit number is divided across one 
warp so that each thread has its own 32-bit integer. 

The parallel shift function is straightforward: each thread is given an 
equal-sized piece of the large-precision integer. Then each thread except for 
Thread 0 grabs a copy of the integer at \texttt{threadID  - 1}.
The variable \texttt{threadID} refers to a value between 0 and 31 and corresponds 
to a thread in a warp. Each thread shifts its value once and 
uses its copy of the adjacent integer to determine if a bit has shifted 
between threads. This procedure is outlined in Algorithm \ref{alg:parShift}.
\begin{algorithm}
   \label{alg:parShift}
   \KwIn{$x[32]$ is a 1024-bit integer represented as an array of 32 \texttt{int}s, $threadID$ is the 0-31 index of the thread in warp.}
   \nl\eIf {$threadID \neq 0$} {
      \nl$temp\gets x[threadID - 1]$\;
   }{
      \nl$temp\gets 0$\;
   }
   \nl$x\gets x>>1$\;
   \nl$x\gets x \:\text{OR}\: (temp << 31)$\;
   \caption{Parallel right shift}
\end{algorithm}

The parallel subtract uses a method called \emph{carry skip} from 
\cite{fujimoto2009high}. First, each thread subtracts its piece and sets the 
``borrow" flag of \texttt{threadID - 1} if an underflow occurred. Next, each 
thread checks if it was borrowed from and if so, decrements itself and clears
the flag. Then, if another underflow occurs, the borrow flag at
\texttt{threadID - 1} will be set. This continues until all the borrow flags
are cleared. An outline can be found in Algorithm \ref{alg:parSub}.

\begin{algorithm}
   \label{alg:parSub}
   \KwIn{$x$ and $y$: two 1024-bit integers, $threadID$ is the 0-31 index of the thread in warp.}
   \nl$x[threadID]\gets x[threadID]-y[threadID]$\;
   \nl\If {underflow} {
      \nl set $borrow[threadID - 1]$\;
   }
   \nl\Repeat {all $borrow$ flags are cleared} {
   \nl\If {$borrow[threadID]$ is set} {
      \nl$x[threadID]\gets x[threadID] - 1$\;
         \nl\If {underflow} {
            \nl set $borrow[threadID - 1]$\;
         }
         \nl clear $borrow[threadID]$\;
      }
   }
   \caption{Parallel subtract using ``carry skip"}
\end{algorithm}

The parallel greater-than-or-equal has each thread check if its integers are 
equal. If this is the case, then it sets a position variable shared by the warp 
to the minimum of its \texttt{threadID} and the current value stored in the position variable.
This is done atomically to ensure the correct value is stored. Finally, all 
the threads do a greater-than-or-equal comparison with the integers specified
by the position variable. This function is outlined in Algorithm \ref{alg:parGEQ}.
\begin{algorithm}
   \label{alg:parGEQ}
   \KwIn{$x$ and $y$: two 1024-bit integers, $threadID$ is the 0-31 index of the thread in warp.}
   \KwOut{$True$ if $x \geq y$; else $False$.}
   \nl\If {$x[threadID] \neq y[threadID]$} {
      \nl$pos\gets \text{atomicMin}(threadID, pos)$\;
   }
   \nl\Return $x[pos] \geq y[pos]$
   \caption{Parallel greater-than-or-equal-to}
\end{algorithm}

\subsection{Computational Complexity}
\label{subsec:compcomp}
The computational complexity of the binary GCD algorithm has been shown by 
Stein and Vall$\acute{\text{e}}$e (cf. \cite{stein1967computational}, 
\cite{vallee1998complete}) to have a worst case complexity of $\mathcal{O}(n^2)$ 
where $n$ is the number of bits in the integer. The worst case is produced 
when each iteration of the algorithm shifts one of its arguments only once. 
Since for this application $n$ is fixed at 1024 bits, the complexity of a 
single GCD calculation can be considered to be constant time for the worst case.

To compare all the keys together, the amount of GCDs that must be calculated 
grows at a rate of $k^2$, where $k$ is the number of keys.

\subsection{Theoretical Speedup}
\label{subsec:theory}
Maximum speedup is defined as follows:
\begin{equation}
   \mbox{Max Speedup} = \frac{1}{1 - P}
   \label{eq:speed}
\end{equation}

where $P$ is the percentage of the program's execution that can be parallelized.
This percentage is a function of the number of keys the program needs to 
process, and is calculated in Equation \ref{eq:percent}.
\begin{equation}
P = \frac{t \cdot g}{t \cdot g + r \cdot k}
   \label{eq:percent}
\end{equation}
where
\begin{itemize}
   \item $t$ = time to process a single GCD
   \item $g$ = total number of GCD calculations
   \item $r$ = time to read a single key
   \item $k$ = total number of keys
\end{itemize}

Since $g$ will increase significantly more rapidly than $k$, $P$ (based on 
equation \ref{eq:percent}) will approach $1$ as $k$ approaches 
$\infty$. This relationship can be observed in Figure \ref{fig:parPercent}.

\begin{figure}
   \centering
   \includegraphics[width=3.5in]{chart_7.png}
   \caption{Total percentage of CUDA implementation that is parallel}
   \label{fig:parPercent}
\end{figure}

\section{Problem Description}
\label{sec:probdesc}
The RSA weakness described above demands that each key in a set be 
compared with each other key to determine if a GCD greater than 1 exists for 
any pair. Given a known set of keys, it is not known before processing 
the keys which will be likely to have a GCD greater than 1; therefore, there 
is no way to eliminate comparisons between specific pairs. The natural 
organization to fulfill this requirement is a comparison matrix of the all 
keys. Each location in the matrix corresponds to a comparison between two 
keys.

\section{Implementation}
\label{sec:impl}
\subsection{Problem Decomposition}
\label{subsec:probdecomp}
Initially, the comparison matrix seems to be an $n^2$ solution. However, the 
diagonal of the matrix created consists of unproductive GCD calculations since 
these entries would compare each key with itself. Furthermore, the matrix is 
symmetrical over the diagonal. Thus, only the comparisons comprising one of 
the triangles needs to be performed. Specifically, 
\begin{equation}
   \label{eq:gcd}
   \mbox{Total number of GCD compares} = \sum_{i=1}^k i
\end{equation}
This reduction in number of overall compares decreases the work 
performed significantly, shown in Figure \ref{fig:compvkeys}. 

\begin{figure}
   \label{fig:compvkeys}
   \centering
   \includegraphics[width=3.5in]{chart_6.png}
   \caption{Number of comparisons needed vs. total number of keys in set}
\end{figure}

\subsection{Grid Organization}
\label{sec:gridorg}
One of the most important aspects of any CUDA implementation is the 
organization of the thread and block array to ensure that the architecture is 
appropriately used to its full potential. The threads array in this 
implementation was organized using 3 dimensions. The $x$ dimension represented 
the sectioning of a 1024-bit value into individual 32-bit integers, of which 
there are 32. 
\begin{displaymath}
   \frac{1024 \:\mbox{bits per key}}{32 \:\mbox{bits per integer}} = 
   32 \;\mbox{integers per key}
\end{displaymath}
The remaining dimensions, $y$ and $z$, were set to 4, resulting in a block of 
512 threads. This design decision was experimentally determined. See 
$\S$\ref{sec:Occupancy} for details about Occupancy optimizations.
\begin{displaymath}
   32 \cdot 4 \times 4 = 512
\end{displaymath}
This ensured that each block remained square for algorithmic 
symmetry and simplicity. The $y$ and $z$ dimensions corresponded to how many 
specific keys within the list of all keys were being compared per block. 
Thus, two 1024-bit keys were loaded into each 32-thread warp, which was 
then processed simultaneously as a single comparison. The $x$ dimension was 
chosen for two reasons: 1) so one thread in this dimension would represent 
each of the 32-bit integers inside the key and 2) because there are 32 threads 
in a single warp. Therefore, this thread-array organization ensured that 
compares were done using two entire keys (separated into 32 pieces) that were 
scheduled to the same warp. This eliminated warp divergence since every warp 
was filled and executed with non-overlapping data.

Blocks were arranged in row-major order based on the key comparisons that they 
held. The formula for the number of blocks, $B$, needed for a vector of keys of 
size $k$ can be seen in Equation \ref{eq:blocks}.
\begin{equation}
   \label{eq:blocks}
   \sum_{i=1}^{\left\lceil \frac{k}{4} \right\rceil}i = B
\end{equation}

The limit for a grid in a single dimension is $2^{16} - 1 = 65535$ 
and limits the amount of keys that can be processed to $1444$. To increase 
the number of blocks available for computation, a second grid dimension was 
added. This increased the theoretical maximum number of keys per kernel 
launch as seen in Equation \ref{eq:maxKeys}.
\begin{equation}
   \label{eq:maxKeys}
   \begin{split}
   \sum_{i = 1}^{\left\lceil\frac{k}{4}\right\rceil} i & \leq {\left(2^{16} - 
   1\right)}^2\\
   k & \leq 370716\\
   \end{split}
\end{equation}

\subsection{Shared Memory}
\label{subsec:shared}
Shared memory was used to load the necessary keys from global memory. Two 
arrays were created in shared memory, representing the thread-array; both 
3-dimensional, $32\times4\times4$ and had an integer loaded into each 
available space. Each array represented which integers would be compared at 
each location in the matrix. A side effect of this organization was that each key would be 
repeated 4 times within its integer array. However, this greatly simplified 
the GCD algorithm so that only a look-up into each array was needed. Since 
shared memory was not the limiting factor for occupancy, it was not a priority 
to optimize this aspect of the design and implementation. 

Shared memory was also used within the GCD algorithm, specifically in the 
greater-than-or-equal-to function, and the subtract function. In the 
greater-than-or-equal-to function, a single integer was allocated for each 
comparison within a block. Within the subtract function, shared memory was 
utilized to represent the borrow value for each integer. 

\subsection{Occupancy}
\label{sec:occupancy}
Each SM can be assigned multiple blocks at the same time as long as there are 
enough free registers and shared memory available. The ratio of active warps 
to the maximum number of warps supported by a SM is called \emph{occupancy}. 
On the Fermi architecture, the maximum occupancy is achieved when there are 48 
active warps running on a SM at one time. Greater occupancy gives a SM more
opportunities to schedule warps in a fashion to hide memory accesses, thus,
saturating a SM with many warps decreases performance impact. CUDA Fermi cards
have a total of 32768 registers and 49152 bytes of shared memory per SM. The
implementation here uses 17 registers and 4762 bytes of shared memory per
block and therefore results in a maximum occupancy of 100\%.

By using the CUDA occupancy calculator provided by NVIDIA (cf. 
\cite{nvidia2012gpu}), a plot can be formed comparing the threads per block 
with occupancy. To maintain the same block organization outlined above, the 
block dimensions can be $2\times2, 3\times3, 4\times4, 5\times5$ or 128, 228, 
512, 800 threads, respectively. Table \ref{tab:occupancy} shows the calculated 
occupancy for these block sizes. A block size of 512 threads was chosen 
because it results in the greatest occupancy and thus the best performance. 

\begin{table}
   \label{tab:occupancy}
%\tbl{Table giving occupancy for various block dimensions\label{tab:occupancy}}{
   \centering
   \begin{tabular}{l|rrrr}
      Threads per block & 128 & 288 & 512 & 800\\
      \hline
              Occupancy & 67\% & 94\% & 100\% & 52\%\\
   \end{tabular}
   \caption{Table giving occupancy for various block dimensions}
\end{table}

\subsection{Bit-vector}
\label{subsec:bitvector}
The initial approach was to allocate a large, multi-dimensional array of 
integers that would hold the results of the CUDA GCD calculations. This was 
allocated to the GPU, so each thread could have access as needed; however, 
since the number of results grew at $n^2$, the lack of scalability in this 
approach was quickly apparent. Additionally, performance decreased due to the 
large array that was being sent over the PCIe bus. Memory transfers to 
the GPU are slow, and must be minimized.

After more careful consideration, a new approach was implemented. There would 
only be a single bit allocated per key-compare to mark whether or not the pair 
had a GCD greater than 1. In this way, only 2 bytes (16 bits = 1 bit per 
compare) were necessary per block ($4\times4 = 16$ compares per block), as 
opposed to the previous $16 \cdot 32 \cdot 4 = 2048$ bytes. Despite not having access 
to the answer immediately after returning from the kernel calculation, this 
approach would be more efficient since there would be a theoretically small 
number of keys that actually returned with GCDs greater than 1 (i.e. the flag 
was set). This small set could then be re-processed (GCDs calculated) using a 
different kernel or using a CPU algorithm. Efficiency would also be increased 
due to the time saved in memory transfers since there was significantly less 
memory to transfer before calling the kernel.

\section{Experimental Setup}
\label{sec:expsetup}
\subsection{Test Machine}
\label{subsec:testmachine}
All performance measurements were made on a single machine with an Intel Xeon 
W3503 dual-core CPU and 4 GB of RAM. This machine has one NVIDIA GeForce
GTX 480 GPU with 480 CUDA cores and 1.5 GB of memory. The CUDA driver 
version present on the machine is 4.2.0, release 302.17, the runtime version is
4.2.9. The CUDA compute capability is version 2.0, and the maximum threads per 
block is 1024, with each warp having 32 threads.

\subsection{Reference Implementations}
\label{subsec:refimpl}
In order to check the accuracy of the final implementation, as well as to 
provide a point of comparison for benchmarking, two reference implementations 
of this exploit were created. Each was able to use the same format key 
databases (described in $\S$\ref{sec:testSets}). 

The first implementation was written purely in Python using the open source
PyCrypto cryptography library. % 2.7.1. 
%Since Python's 
%\texttt{long} type can contain integers of arbitrary size, and its math library 
%can perform operations on these values, it was most straightforward to 
%implement the entire program using these tools. The open source PyCrypto\footnote{https://www.dlitz.net/software/pycrypto/} 
%library (version 2.5) was used to import and process the RSA key pairs. 
This implementation was able to perform the entire exploit, from finding 
weak 1024-bit RSA public keys through generating the discovered private keys. 
%It was used 
%primarily as a sanity check for the database generation in order to 
%determine if the keys generated as expected. 
This implementation was not used for performance comparison as it was
dissimilar to the other two implementations.

A sequential version of the binary GCD algorithm was implemented to serve as a 
second validation tool for the CUDA implementation. This version sequentially 
processed the same input as both other implementations and produced output of the 
same format. Comparison with this implementation ensured that unexpected errors 
did not result merely from processing the data in parallel.

\subsection{Test Sets}
\label{sec:testsets}
In order to conduct meaningful tests, it was necessary to use an identical 
data set in all tests. To facilitate this, a tool was written in Python to 
generate both regular and intentionally weak RSA key pairs using PyCrypto and store them in an SQLite3 database. All keys were generated with a 
constant $e$ of 65537, chosen because this was discovered to be a commonly used 
value (cf. \cite{lenstra:ron}).

The generation process produced a database of RSA key pairs. 
Intentionally weak keys were evenly distributed. 

In order to generate a weak key, this program would generate an initial 
normal RSA key but save the prime used for $p$. For each subsequent bad key, 
$p$ would be replaced with this constant, and $n$ was recalculated. The 
result was that each weak key would have a GCD greater than 1 when 
tested against any other weak key. 

Using this tool, it was possible to build arbitrarily large test sets with a 
known number of keys exhibiting the weakness. When these databases 
were processed using any of the reference implementations, the 
discovered number of weak keys could be directly compared with the 
number of keys expected to be found. This allowed both repeatable testing to 
measure run time, and a method to validate the parallel algorithm was indeed 
finding GCDs as expected. 

\section{Results}
\label{sec:results}
The accuracy of the parallel implementation was verified against the 
sequential implementation by using identical test data sets with known 
weak keys. Since both implementations found the same set of compromised keys,
it was validated that these two implementations 
were internally consistent. Furthermore, both matched the results of the 
separate Python reference implementation: supporting the assertion of accurate 
functionality. The speedup of the CUDA implementation (seen in Figure 
\ref{fig:speedup}) was calculated by comparing its run time with that of the
sequential implementation. Compare this with Figure \ref{fig:parPercent}:
this similarity is evidence of the implementation presented here matching
with theoretical expectations.

\begin{figure}
   \label{fig:speedup}
   \centering
   \includegraphics[width=3.5in]{chart_1.png}
   \caption{Speedup of CUDA Implementation to Sequential C++}
\end{figure}

Figure \ref{fig:speedup} shows that speedup increases dramatically with 
the number of keys until about 2000 comparisons. At this point, the GPU
becomes saturated with enough blocks to fully occupy all of the SMs. Speedup
remains constant at 27.5 for up to 200000 keys. We have no data beyond this
number of keys due to the very long run time of the sequential implementation. 

%The increase in the number of 
%comparisons affects the sequential performance significantly more than the CUDA 
%performance, which is particularly apparent in Figure \ref{fig:runtimes}.
%
%\begin{figure}
%   \centering
%   \includegraphics[width=3.5in]{chart_4.png}
%   \caption{Run-time comparison}
%   \label{fig:runtimes}
%\end{figure}

%\begin{table*}
%%\tbl{Average run-times of sequential and CUDA implementations\label{tab:runtimes}}{
%   \centering
%   \begin{tabular}{|l|*{7}{r}|}\hline
%      Number of Keys     & 200 & 400 & 800 & 1000 & 2000 & 4000 & 10000\\
%      \hline
%      Sequential Average (sec) & 8.10 & 32.44 & 129.83 & 202.97 & 810.86 & 8242.75 & 20258.75\\
%      CUDA Average (sec) & 1.97 & 2.71 & 5.47 & 7.64 & 26.31 & 104.15 & 609.11\\\hline
%      \textbf{Speedup} & 4.1 & 11.96 & 23.76 & 26.58 & 30.81 & 31.13 & 33.26\\\hline
%   \end{tabular}
%   \caption{Average run-times of sequential and CUDA 
%     implementations\label{tab:runtimes}}
%\end{table*}

\begin{table*}
%\tbl{Average run-times of sequential and CUDA implementations\label{tab:runtimes}}{
   \label{tab:runtimes}
   \centering
   \begin{tabular}{|l|*{5}{r}|}\hline
      Number of Keys        & 20   & 200  & 2000   & 20000    & 200000 \\ \hline
      Sequential Time (sec) & 0.13 & 5.59 & 550.69 & 55121.86 & 185551.91\\
      CUDA Time (sec)       & 0.21 & 0.31 & 20.21  & 2005.09  & 6748.23\\\hline
      \textbf{Speedup}      & 0.6  & 18.0 & 27.2   & 27.5     & 27.5\\\hline
   \end{tabular}
   \caption{Run-times of sequential and CUDA implementations}
\end{table*}

%One run of the CUDA implementation was performed using a data set of 200,000 
%keys. Using Equation \ref{eq:gcd}, this many keys required 20,000,100,000 
%comparisons. This run of the program took approximately 114 minutes to 
%complete, which translates to 
%\begin{displaymath}
%   \begin{split}
%      \frac{20,000,100,000 \;\text{comparisons}}
%      {114 \;\text{min}} = 2.923 \times 10^6 \frac{\text{comparisons}}{\text{sec}}
%   \end{split}
%\end{displaymath}
%Each comparison consists of performing the binary GCD calculation between two 
%1024-bit values.

\section{Conclusion}
\label{sec:concl}
A large speedup resulted directly from writing a CUDA 
implementation when compared to the sequential implementation. Many 
more keys are able to be compared in a given amount of time using the CUDA
implementation. 

A tool was developed to efficiently and completely compare a list of 1024-bit 
RSA public keys, avoiding repetition and unnecessary work. This 
tools allows an increased number of keys to be compared 
compared to prior work, in turn allowing overall execution time to decrease due 
to the increased parallelism.

The tool described in this paper offers significant advantages over other GCD 
algorithms in CUDA, and practically applies this for comparison of 1024-bit 
RSA keys in order to test for a particular weakness. There also exist 
several areas where the implementation would benefit from further 
investigation and development including application of the GCDs, and 
expansion to iterative kernel calls in order to handle even larger sets of 
keys to compare.

\section{Future Work}
\label{sec:future}
The primary limiting factor of this implementation is the amount of memory 
available on the GPU. Since all key combinations must computed to expose any 
potential weakness, the kernel was structured to take a single vector of 
keys and perform all possible comparisons. In order to process more keys, 
either a GPU with more memory must be used, or the algorithm must be modified 
in order to use multiple, iterative kernel launches. The iterative kernel 
approach would require memory to be separated into two sections that could 
each be filled with subsections of the large, complete array of keys. All 
comparisons would then be performed between the two subsections by calling the 
kernel that is currently implemented. Upon returning from the kernel, the data 
in one subsection would be shifted, and all the compares would then be done 
for those two sets of keys. This process would continue until both vectors 
have iterated over all keys: specifically, the kernel call would reside in a 
pair of nested \texttt{for} loops. This change would allow the implementation 
presented here to process an arbitrary number of keys.

To further enhance the above proposed addition, asynchronous memory 
transfers could also be added to the implementation. When combined with 
multiple kernel launches, a significant portion of the memory I/O (which is one 
of the main limiting factors of performance using the GPU) would be able to be 
masked by simultaneously processing the data currently on the GPU while new 
data is being copied onto it.

An aspect that was originally intended for this project, but was not 
implemented was to have the CUDA kernel return the actual GCD of any keys that 
were found to be ``weak'' in the sense there existed a GCD greater than 1. 
Since a large majority of the GCDs found by our implementation are equal to 
one, memory is wasted if all the results are transferred back to the host. The 
most memory efficient solution would include dynamically allocating memory for 
any significant results on the device. This would remove the recalculation step 
in the current implementation needed to produce private keys.

Recently NVIDIA has released information about a new GPU architecture called 
Kepler\cite{nvidiakepler}.
The Kepler architecture introduces new features that may increase the 
performance of this implementation. \newpage A feature known as Dynamic
Parallelism  allows a CUDA kernel to launch new kernels from the GPU. This
would allow dynamic allocation of block sizes for different areas of the
comparison matrix and remove idle threads from the kernel. Hyper-Q is a new
technology that manages multiple CUDA kernels from multiple CPU threads. With
the current Fermi architecture, only one CUDA kernel may run on the device at
one time. This can lead to under utilization of the GPU hardware. An approach
using multiple CPU threads, each running their own CUDA kernel, could greatly
increase throughput.

The final missing component of this implementation would be to complete the 
algorithm and use the GCDs which are being calculated to generate RSA private 
key pairs. In order to do this, a heterogeneous multi-process approach may be 
most straightforward. After the parallel run completes, the bit-vector of 
results would be examined to find the key pairs which produced GCDs greater 
than one. In these cases, the GCD would be recomputed, and used as either $p$ 
or $q$. Once this was done, the missing prime would be computed, then based on 
this, $d$ would be found. The resulting RSA public/private key pair could be 
exported or stored for later verification.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Background}
\label{ch:background}
RSA is an asymmetric key encryption scheme. Keys come in matched pairs:
public keys and private keys. Using the same algorithm, information encrypted
with the one key, can be decrypted with the other and vice versa. A party must
keep the private keep secret, but the public portion of the key can be seen and
used by anyone in the world. To generate a pair, an algorithm is performed on
two randomly-generated prime numbers whose product is of a certain bit-length
(e.g. 1024 bits). These are the primes that our work looks to discover
inadequacies with.

The work presented by Lenstra et. al. \cite{lenstra2012ron} shows that around
0.2\% of RSA keys collected from sources including the SSL Observatory suffer
from inadequate random prime generation. They show that when the primes used to
build an RSA public-private key pair are not actually random, factoring does
not need to be done to break the keys. Instead, only a greater common divisor
needs to be calculated between two RSA moduli. When anything greater than 1 is
found as a result of this calculation, the keys can be considered broken and
offer no protection. This is because once a GCD greater than 1 is found, the
private keys can be generated from the publicly available information. The
vulnerability is explained in detail in Figure \ref{fig:vuln}.

\begin{figure}
   \label{fig:vuln}
   \centering
   \includegraphics[width=\linewidth]{vulnerability.png}
   \caption{Explanation of poor-prime vulnerability}
\end{figure}

A tool by Scharfglass et. al. \cite{scharfglass2012breaking} performs a
parallelized version of this GCD calculation (using the binary GCD algorithm)
between every pair combination in a given set of keys. It structures these
calculations so that as many as possible can be done at once. To increase
parallelism, and because the keys are so large, they are broken up into 32
chunks, which are also operated on in parallel using the GPU. Through these 2
levels of parallelism, a speedup of 27.5 is achieved. This tool provides a
practical tool for checking a given set of keys for the vulnerability we are
focusing on.


\section{Motivation}
\label{sec:motivation}
Scharfglass et. al. use NVIDIA's CUDA framework to significantly speedup a
check for the vulnerability
presented in \cite{lenstra2012ron}. The work in \cite{scharfglass2012breaking}
lacks any context or explanation of usage for RSA keys in general, and although
their data set is security- and encryption-focused, no discussion is given to
the security aspects and the potential impacts of the work. Some research was
done in an attempt to add context to the work, and the implications it might
have.

Most initial research yielded general statements that did not explain or give
appropriate context to RSA (statements including ``\dots all over the
Internet\dots" were accurate, but not informative). More in-depth research
yielded two primary use-cases of RSA: Digital Rights Management (DRM) and
Secure Socket Layer (SSL) and Transport Layer Security (TLS) Internet
security protocols. Other uses included password alternatives (e.g. ssh
connections or command line interface tools like \texttt{git}) but these are
more difficult to collect data for and analyze since they tend to be done on
an individual basis, and are managed by individual users.

\subsection{Digital Rights Management}
Digital rights management is a protocol used to secure the usage and
distribution of various types of media content. It is adopted by content
providers and device manufacturers to ensure users don't misuse or wrongfully 
share protected, copyrighted content.
The RSA Association proposed a protocol that could be applied to various types
of media content that secured its usage \cite{rsa2004announces,
rsa2004supports}. These announcements argued that using RSA for DRM offered
benefits to the content providers, the device manufacturers, and, arguably, the
consumers of the content. It apparently allowed device manufactures and
content providers to ensure proper usage of protected content while allowing
users to consume and playback their rightfully-owned media on an array of
their own devices.

\subsection{Internet Security and Certificates}
Internet security relies on a particular protocol called SSL and its successor
TLS. These protocols define how to securely transfer information over the
Internet by using encryption and signing mechanisms. One aspect of both the SSL
and TLS protocols involve certificates to ensure the expected party is
actually being communicated with. These certificates provide information
about a server that a user is connected to (e.g. Amazon or Google)
and is signed by a certificate authority (e.g. VeriSign). These certificates
are also encrypted with a subject's (e.g. Amazon) public key to ensure that
the entity is who they say they are. Furthermore, the public key is then used 
to transfer information back to the subject in a secure way. Figure
\ref{fig:tls} describes the TLS process between a client and server. 

The most widely used type of certificate is the X.509 certificate, whose current
revision is 3. A breakdown of the major sections of the certificate is shown in
Figure \ref{fig:x509}. The ``Subject Public Key Information" section of the
certificate holds the relevant data (the subject's RSA public key) for our work.
The public key portion of the certificates can actually use one of several
algorithms defined in the specification. RSA is by far the most popular,
accounting for over half of the found certificates (as shown in Figure
\ref{fig:certs}). Other options include DSA and Diffie-Hellman. For more
detailed information about X.509 certificates and analysis on their
infrastructure, see \cite{holz2011ssl}.

\begin{figure}
   \label{fig:x509}
   \centering
   \includegraphics[width=0.5\linewidth]{x509.png}
   \caption{X.509 Fields\cite{holz2011ssl}}
\end{figure}

\begin{figure}
   \label{fig:tls}
   \centering
   \includegraphics[width=0.5\linewidth]{tls.jpg}
   \caption{TLS Overview\cite{tlsconcepts}}
\end{figure}

\subsection{Data Collection}
\label{subsec:datacol}
The proposed protocol given by the RSA actually modeled the certificate model
used by SSL/TLS. For this reason, the SSL certificate model was the primary
focus of the data collected. Moreover, the data set of RSA keys in SSL
certificates is much larger than DRM implementations that use RSA. Thus, the
data set that was collected and then analyzed consisted entirely of X.509
certificates.

Since a primary motivation of this work is the potential impact that vulnerable
RSA keys may have, a data set representative of a large portion of traffic was
desired. A survey was done over 2007-2010 \cite{labovitz2011internet} that
gathered some useful data on the Internet as a whole. Relevant to web traffic,
this study showed that together, the major content data networks (CDN) (i.e.
Google, Facebook, Amazon, etc.) account for nearly 17\% of web traffic.
Specific breakdowns and conversions can be found in Table \ref{tab:traffic}.

\begin{table}
\label{tab:traffic}
\centering
\caption{Major CDN Traffic (2010)}
\begin{tabular}{|>{\raggedright}p{0.4\linewidth}
                |>{\raggedright}p{0.2\linewidth}
                |>{\raggedright\arraybackslash}p{0.2\linewidth}|}\hline
   \textbf{CDN} & \textbf{Percent of all Internet traffic} & \textbf{Approx. percentage of web traffic}\\ \hline
Google & 7 & 12.72\\ \hline
Facebook & 0.45& 0.818\\ \hline
Amazon CDN (NASA/JPL, PBS,\dots) & 0.55 & 1\\ \hline
EdgeCast (Yahoo!, Break, Imgur,\dots) & 0.5 & 0.909\\ \hline
LeaseWeb (Heineken, Starbucks,\dots) & 0.8 & 1.454\\
\hline\end{tabular}
\end{table}

Alexa was used to find websites with large amounts of
traffic\footnote{http://www.alexa.com/}, as it serves as one of the Internet's
most prevalent sources for traffic information. In addition to individualized
site traffic data, Alexa provides a daily-updated list of the top one million
websites ordered by
traffic\footnote{http://s3.amazonaws.com/alexa-static/top-1m.csv.zip}. In
regards to the previous point concerning CDNs and traffic breakdowns, it was
noted that all web sites hosted by the major CDNs from
\cite{labovitz2011internet} are present in the top one million list.
Additionally, the vast majority of sites from the top one million list were
not provided by the CDNs mentioned in \cite{labovitz2011internet}. This means
that a much larger percentage than 17\% is actually represented by the sites
in the list, although quantifying precisely how much becomes difficult, and
lies beyond the scope of this project.

A Python script was used to parse the CSV file provided by Alexa. Each
extracted URL was visited on port 443 (corresponding to ``https://") using 
\texttt{openssl}. If a site responded on that port, it provided an X.509
certificate to be parsed and verified. Most of the time, this is done by a
user's web browser which has certificate authorities' private keys hidden
within. However, since the work here was not concerned with the actual web
content, the certificate was simply saved. When certificates were found, the
``Subject Public Key Algorithm" section was inspected. If this contained some
form of RSA, \texttt{openssl} was used again to extract the RSA key and save
it into a PEM file (a standard format for saving public and private key
information in). Additionally, since part of this work's motivation lies in
expanding the work done in \cite{scharfglass2012breaking}, the keys were also
stored into a SQLite3 database so that later retrieval by the tool was trivial.

After writing an initial implementation of the Python script, it was realized
that the collection process was too slow. Therefore, some of Python's
multi-process capabilities were utilized to speed up the collection of data.
Eight processes were spawned to perform the work described above. Seven of the
processes read URLs from a deque (double-ended queue) and attempted to obtain
an X.509 certificate and RSA key from each. When found, these keys were added
to a queue. These data structures not only offered convenient structures for
organizing the data, but were also thread-safe and could be used with our
updated implementation. An eighth thread read keys from the queue and entered
them into the SQLite3 database. This reimplementation offered a 15$\times$
speedup over the previous, na\"{i}ve approach.

\subsection{Data Analysis}
\label{datanalysis}
After collecting data from each of the top one million Alexa sites, some
surface-level analysis was performed in order to gain an overview of the
security of these one million sites. This analysis included overall security
breakdowns (i.e. whether a site used any kind of security) and
bit-length breakdowns of the RSA keys that were found. This data analysis can
be found in Figures \ref{fig:certs} and \ref{fig:bits}. Table \ref{tab:bits}
offer specific breakdown for bit-lengths, and offers details that cannot be
seen in the figure.

\begin{figure}
   \label{fig:certs}
   \centering
   \includegraphics[width=0.75\linewidth]{cert_security.png}
   \caption{Presence of RSA key or other security in Alexa top 1M sites}
\end{figure}

Interestingly (and slightly worrisome), is that the majority of web sites in
this top one million list did not even respond on port 443, implying that
they do not allow any option for secure traffic to their site. Nearly 60\% of
websites in the list fell into this category.

Also of note are the specifics of bit-lengths found. It is a surprising data
set for various reasons. First, bit-lengths as small as 384, 512, and 768 were
not expected as keys of this size were factored years ago
\cite{rsa2007challenge}, and cannot be expected to offer much
security\footnote{Interesting note: The author attempted to visit the sites
384-bit keys. Google Chrome did not find a web page on port 443 for any of
the sites.}.

On the other hand, the high number of longer bit-lengths was also surprising,
but comforting. The fact that the vast majority of keys were 2048-bit was not
expected, but means that a large number of high-traffic websites have adequate
(for now) encryption. Moreover, there were quite a number of sites with 4096,
and, even more surprisingly, 8192-bit.

\begin{figure}
   \label{fig:bits}
   \centering
   \includegraphics[width=0.75\linewidth]{bit_length.png}
   \caption{Breakdown of Alexa top 1M sites' RSA keys by bit-length}
\end{figure}

\begin{table}
\label{tab:bits}
\centering
\caption{Alexa top 1M X.509 RSA bit-length}
\begin{tabular}{|r|r|}\hline
\textbf{Bits} & \textbf{Count}\\\hline
384 & 4 \\ \hline
512 & 309 \\ \hline
768 & 4 \\ \hline
1024 & 46736 \\ \hline
2048 & 168391 \\ \hline
2432 & 37 \\ \hline
3072 & 17 \\ \hline
4096 & 4511 \\ \hline
8192 & 14 \\ \hline
other & 44 \\ \hline
\end{tabular}
\end{table}

Further analysis was done using the tool described in
\cite{scharfglass2012breaking}. This tool performs a complete check of all
RSA keys in a given set for the previously mentioned vulnerability described
in \cite{lenstra2012ron}. This check is done using NVIDIA's CUDA framework to
parallelize the work as much as possible which allows for a practical security
validation tool. At the time of writing, the tool's current implementation is
limited to 1024-bit RSA keys. Due to this limitation, the data set was
reduced to 46,736 RSA keys. Due to the $O(n^2)$ nature of this problem,
this check resulted in $1,092,150,216$ comparisons. The run of the tool using
this data set took 138 minutes, 37 seconds or $131,305$ comparisons per second.
After applying the tool to this set of keys, it was found that no keys were
susceptible to the vulnerability (insofar as none of the keys in the set
shared primes with a given key). Although no vulnerabilities were detected,
this does not mean the set is completely secure from the previously mentioned
vulnerability. Since such a limited data set was used, and the security tool
relies on large sets to accumulate many primes, the analysis presented can
only be considered an initial pass.

\subsection{Implications}
\label{subsec:implications}
For the same reasons the data set was chosen to be X.509 certificates, presence
of the vulnerability mentioned here would have the largest effect on Internet
security: specifically in the certificate infrastructure. If servers were
unknowingly providing compromised public keys in their certificates, their
traffic could offer no security if this became known. This could offer
ill-effects since the vulnerability is agnostic of the type of data being
being encrypted.

Additionally, the password-alternatives that were mentioned briefly before
could be particularly impacted. If a system was generating many new RSA keys
(e.g. a user is building new accounts for various websites, and tying RSA keys
to them) with inadequate primes, potentially all these accounts for this user
could end up compromised, despite the sense of stronger security.

Along these lines, a system administrator could potentially suffer from similar
issues. If many machines and key-pairs are being setup and generated as part of
an initial setup process for a new network of users, poor random number
generation could have severe consequences when done at scale. This would likely
create many new, vulnerable keys at once. By being aware that this problem
could exist, and that a tool such as \cite{scharfglass2012breaking} exists, an
insecure set of users could be avoided or detected much earlier than would have
otherwise occurred.

% probably unnecessary at this point
%\subsection{Future Work}
%As mentioned, this survey suffers from various limitations. Some immediate
%steps are recommended to make the analysis more complete. Primarily, a larger
%data set of any kind must be used with the tool in
%\cite{scharfglass2012breaking}. This could come in several ways. First,
%additional keys could be generated and added to the set of $46,736$ so that more
%prime values are present in the pool of primes. Second, more primes could be
%collected from other certificates online. The tool gains functionality as the
%number of keys increases, so the more keys used, the more complete the results.
%
%Furthermore, extending the functionality to 2048-bit keys would offer several
%advantages over the present work. First, a much more accurate analysis of
%high-traffic X.509 certificates could be gained since this was the largest
%portion of the data collected (see Figure \ref{fig:bits}). Second, it would
%offer a much larger data set, which, as previously mentioned, adds
%effectiveness to the tool.
%
%The survey could also be expanded to include public keys from the 
%previously-mentioned self-maintained keys (e.g. password alternatives). If
%these keys were accessible from some set of services, it could contribute to
%the data sets significantly. This would especially hold true if there existed
%ways to correlate where specific keys originated form (i.e. which machine
%generated them). This way, when vulnerable keys were found, a machine could be
%flagged as potentially insecure for key generation.
%
\subsection{Conclusions}
\label{subsec:conc}
This survey determined that the primary use of RSA keys in practice is the
signing of SSL certificates. A data set was gathered that accounts for a
significant amount of certificates in the context of web traffic. Some
surface-level analysis was done to gain a general understanding of the
collected data. A high-performance tool was then applied to a subset of the
data to check for a vulnerability within the data subset. No vulnerabilities
were found; however, due to limitations in the tool used, as well as
limitations in the data set, this cannot be considered a complete analysis
and requires further investigation.

Though limited, this survey does successfully add real-world application
context to the work done in \cite{scharfglass2012breaking}. Specific
implications are able to be draw from the practicality of the tool, and more
tangible examples of what vulnerabilities in RSA keys looks like have been
realized.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related Works}
\label{related}

\section{The Vulnerability}
\label{sec:vuln}
An RSA vulnerability was discovered and detailed in \cite{lenstra2012ron} in
early 2012 that forms the basis of the exploit the work presented here builds
upon. Namely, that poor random number generation in RSA keys results in
insecure encryption using these keys. Specifically, a key's private components
can be generated using publicly available information by taking advantage of
the work presented in \cite{scharfglass2012breaking}.

The exploit explored in \cite{lenstra2012ron} makes use of the two prime
values that play a fundamental role in an RSA key. These values must be
sufficiently random such that repeated values are infeasible and mathematically
extremely improbable to encounter due to the scope of the set (this work
focuses on 1024-bit keys). However, on some systems and due to some
less-than-adequate code, this may not always be the case. When these values are
repeated between keys, the greatest-common-divisor algorithm can be applied to
find the shared value between keys. This is significant because the GCD
calculation is significantly less computationally expensive than the
alternative: factoring the large values. With this new information about each key,
they private components are straight-forward to generate since the RSA
key-generation process is public and not difficult to implement.

A large sample set was obtained via the OpenSSL Observatory and by crawling the
Internet for SSL certificates (which use RSA as an encryption scheme). Since it
was thought this would provide an adequate sample set, all these keys were
analyzed. The set contained 1024-bit keys, as well as 2048-bit keys. The
findings were that 0.2\% of these keys were vulnerable to this exploit, and
thus offered no security. This included 2048-bit keys as well, despite the
conception 2048-bit keys are more secure (this would be true if this
vulnerability were avoided).

\section{The Process}
\label{sec:proc}
As mentioned, the vulnerability is exploited by performing greatest common
divisor (GCD) calculations with pairs of keys. The research presented in this
paper looks at how to most quickly and efficiently analyze a large database of
RSA keys and detect if this vulnerability is present within the set.
Traditionally, this would be a tedious process that would likely be infeasible
due to time constraints. Since the calculations are independent of one another
(between pairs of keys) they can be done in parallel, given the proper system.
One such system, and the focus of this work, is NVIDIA's CUDA. This will be
detailed later, however, it relates to work done in \cite{fujimoto2009high}. 

This work optimized a specific version of the GCD (binary GCD). This method
reduces the algorithm to three repeated operations. This algorithm was then
implemented in CUDA for 1024-bit numbers (something CUDA does not offer native
support for). This allows much of each operation to be performed in a parallel
fashion, further optimizing the process.

%
%\section{Completed Areas}
%As a groundwork for the research presented here, the work done in
%\cite{scharfglass2012breaking} will be used extensively. Here, the ideas present in
%both \cite{lenstra2012ron} and \cite{fujimoto2009high} are combined with
%further optimization and parallelization of the data to achieve a significant
%speedup of the overall process (when compared to the sequential runtimes). This
%work makes use of CUDA to parallelize the binary GCD algorithm as presented in
%\cite{fujimoto2009high}, but additionally parallelizes each GCD calculation as
%each is independent of the other (that is, many complete GCD comparisons are 
%performed in parallel). This same approach will be used in the work that follows.
%
%This work, however, does not offer a complete solution and suffers from several
%limitations. The work in \cite{scharfglass2012breaking} does not actually produce
%the private components of the RSA keys found to be vulnerable by the algorithm.
%Furthermore, the set of keys that can be used in the algorithm as presented, is
%limited by the memory present on the GPU. To offer a complete solution, this
%must be overcome to allow RSA key-sets of arbitrary size to be used for the
%validation. The usefulness of such work is severely impacted due to these two
%limitations.
%

\section{Multiple GPUs}
\label{sec:multGPU}
As an extension to the work in \cite{scharfglass2012breaking}, multiple GPU support
will be explored. This concept is discussed at length in
\cite{schaa2009exploring}. Here, several GPU configurations are investigated to
analyze how a distributed system may benefit over a single-GPU design.
Work is divided intelligently among the systems in the distributed system to
gain more performance increase than would be possible with a single system.

At the time \cite{schaa2009exploring} was written, single-system,
multiple-GPU configurations were not possible using CUDA over SLI (Scalable
Link Interface, NVIDIA's brand for the interface between multiple GPUs).
However, this is possible now, and will be explored at length in our work.
This configuration offers benefits including transfer speeds over alternative
configurations.

The work done in \cite{thibault2009cuda} helps to make a case for the
additional speedup that making use of multiple GPUs can offer. This work was
done after CUDA had added adequate support for using multiple GPUs
simultaneously, and configurations with two-GPU and four-GPU systems were used.

Significant additional speedups were gained, ranging up to three times speedup
using four GPUs over the single-GPU system (which translates to 100 times
speedup compared to the original, non-GPU benchmark being used). This kind of
speedup offers great optimism in how multiple GPUs may offer significant
increases to performance of the GCD key algorithm.

Another interesting aspect of the work done in \cite{thibault2009cuda} is the
varied domain sizes that were used. The data sets were broken down into several
different configurations which was each tested on the single, dual, or quad-GPU
configuration. This was done to achieve a more accurate answer to how much
faster the multi-GPU system could be. The largest domain set
($1024\times32\times1024$), interestingly, offered the greatest additional
speedup between configurations---the quad-GPU configuration was 3 times faster
than the single-GPU, compared to 2 times for the $256\times32\times256$
domain. This is significant, and implies that exploring different ways to
organize our own data may offer increased advantages when moving to a multi-GPU
system.

\section{Large Data}
\label{sec:largedata}
Since one of our primary goals is to remove the limitation present in
\cite{scharfglass2012breaking} that prevents larger sets of keys from being
analyzed, work investigating how to manage large data sets is relevant.
Specifically, large data sets that must be managed by the GPU.

One such work is \cite{wu2009clustering}. In this work, like our own, the data
is too large to fit within the extremely-limited memory of the GPU. Thus, the
data must be partitioned. In this work, the data was first prepared by
organizing it so that it could be effectively partitioned. There was a
straightforward way of doing so for this application (K-means).

Additionally, some features of the GPU were considered and used, namely
asynchronous memory transfer, a type of multi-tasked streaming. This feature
allows memory transfers to be done between the GPU and CPU while the GPU is
processing data it already has. This can increase performance significantly
when compared to having the GPU move from processing to data transfer (i.e.
having a single tasked thread). Optimization of threads was looked into in this
work as well, as the authors attempted to find how many of these threads would
add the positively impact speedup the most. They found that only 2 threads
(i.e. one processing, one transferring) were needed to gain the greatest
performance benefit. Adding additional streams after this point did not offer
additional benefit.

Decomposition of data is the primary problem when attempting to perform work
with a GPU due to the limitations of current hardware. Therefore, much work and
thought has been given to optimizing the structure of large data sets. Although
a different type of parallelism is looked at in \cite{charles2012chunking}, the
strategies to data decomposition are relevant to most parallel problem in
general, including CUDA. The focus in this survey is matrix multiplication,
a standard parallel problem. The most effective way to chunk the data in this
work appears to be entire rows/columns of the element matrices. This is logical,
and the approach is mimicked (as much as it can be since matrix multiplication
is not perfectly synonymous) in \cite{scharfglass2012breaking}. However, a slight
modification was found to make this strategy most effective: hold the results
in memory until an entire set of iterations completes (all that will fit in
memory) and only then transfer the results back to the CPU. Although this does
not decrease the amount of memory sent from the GPU to the CPU, it reduces the
number of total memory transfers which is often more important. Such fine
details must be found and applied to our work here to further increase our
performance.

\section{Optimization}
\label{sec:opti}
As mentioned, data decomposition and organization around the architecture are
some of the most important (if not \emph{the} most important) aspects of
creating a parallel solution for use with a GPGPU. The work done in
\cite{ryoo2008optimization} gives a detailed overview of how one might optimize
work done with CUDA. It discusses the memory model, and how certain techniques
such as tiling are used to optimize the performance gains. Although some of
these techniques are used in the original implementation found in
\cite{scharfglass2012breaking}, it does not appear that they were used extensively.
It seems that the research and surveying done in \cite{ryoo2008optimization}
can be applied to our work here and our speedup will benefit from their results.

Unfortunately, this work was done before multi-GPU systems were fully available
and accessible using CUDA. The optimization strategies presented in this work
will no doubt be useful; however, other strategies also must be employed that
considered systems with more than a single GPU (such as those mentioned here
from \cite{thibault2009cuda}). Leveraging techniques in both realms will offer
the greatest amount of performance gain.

\section{Similar Algorithms}
\label{sec:simialg}
A similar algorithm that suffers from the same problems that pairwise
comparisons does (each element much be traversed and compared), is search. The
work done in \cite{peters2011fast} explores how search can be optimized for the
CUDA framework. Two approaches are given to the bitonic search algorithm that
is used: the initial, most straight-forward implementation, and another
implementation optimized around the CUDA memory model. The latter offered
significant performance increases over the original. 

A similar approach will be applied to the work that is used from
\cite{scharfglass2012breaking}. Namely, shared memory will be used as efficiently
as possible to allow CUDA to perform at its greatest capacity. This is a vital
concept in general when optimizing for CUDA; however, the work and challenges
from \cite{peters2011fast} are similar to our own, so the implementation
overlaps more than arbitrary examples.

Another important contribution to the field and concepts of distributed systems
is the work presented in \cite{dean2008mapreduce}. MapReduce has become an
extremely useful tool for companies like Google with massive data centers that
must distribute the work to perform as reasonable (real-time) levels. MapReduce
explores how work might be delegated to multiple workers that exist in a
particular distributed system. These concepts apply to the problem of
performing the necessary GCD comparisons using multiple GPUs in a single
system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Validation}
Accuracy and speedup will both be validated to gain a measure of success.

To validate accuracy, a sequential implementation will be compared against.
Much as the work found in \cite{scharfglass2012breaking} performed accuracy
validation, a separate, sequential implementation will be created and ran
alongside the parallel implementation. Since this work will function as an
extension to \cite{scharfglass2012breaking}, the validation process will likely be
identical or even reused. The results each implementation produces must match
for a given set of keys (i.e. both implementations must find the same set of
vulnerable keys given identical sets to start with).

An additional limitation realized in \cite{scharfglass2012breaking} was the runtime
of the sequential version becoming too long for large sets of keys. Since this
is known to be an extremely time-consuming process, it will not always be
performed with larger sets of keys. If correct operation of the parallel
implementation for small (i.e. sets that can be run sequential in a reasonable
amount of time) can be validated using many varying sets of keys of increasing
number, it can be assumed the same level of accuracy will continue for larger
sets that will not be run sequentially. Thus, accuracy will be validated for
sets no larger than 200,000 keys (as in \cite{scharfglass2012breaking}), and
assumed to be accurate (given success of the aforementioned validation) for
sets larger than this.

Speedup validation also involves comparing with a sequential implementation;
however, this time the runtimes will be compared so an overall speedup may be
calculated. Again, much as in \cite{scharfglass2012breaking}, speedup will be
calculated by taking the ratio of the sequential and parallel runtimes.

As seen in \cite{scharfglass2012breaking}, the speedup reached an upper threshold,
namely, 27.5. Since this work will expand upon on that, it seems natural to use
27.5 as a baseline. However, there are some differences to consider. First,
in \cite{scharfglass2012breaking}, sets of keys that could fit entirely in the
GPU's memory were tested and presented. This ensures that the number of memory
transfers to the memory card is 1: this significantly reduces memory transfer
time, and would help increase the speedup. Since one goal of this work is to
allow an arbitrary number of keys to be used in a set, this will likely affect
the overall speedup since memory transfers will become smaller and more
frequent.

On the other hand, another goal is to add multi-card support which would
further increase the total speedup of the final, parallel implementation. Since
we will be looking at how fast additional cards make the process (compared to
the single-card implementation), the sequential runtime will become less
important. It will continue to serve as a baseline to the entire algorithm
though. 

Considering the modifications done, speedup near 25 seems like a reasonable
expectation. Functionality that is expected to decrease the original 27.5, as
well as additions that are expected to increase it will both be researched and
added. Therefore, a speedup in the same area appears to be a fair expectation
of success.

\end{comment}



\chapter{Introduction}
\label{sec:intro}
RSA is a public key encryption scheme which relies on the difficulty of 
factoring large numbers. The algorithm is prevalent throughout security, and is 
specifically used for many web-security applications (see
$\S$\ref{sec:motivation}). An RSA key contains public and private components,
both of which are calculated based on two randomly-generated prime values: $p$
and $q$ (RSA is explained at greater length in $\S$\ref{subsec:rsa}). Ideally,
given the number of possible primes that may be used to construct a 1024-bit
key, no random number generators should reuse either prime, and all keys should
contain unique components. Thus, the likelihood of either $p$ or $q$ being
repeated in a set of keys should be approximately 0. Since in reality, random
number generation is difficult, and often less than random, primes are, in fact,
repeated. Due to this, an individual key may be considered secure by itself, but
when compared to other keys might exhibit a weakness which allows each key's
private components to be calculated entirely from public information.

When considering two keys, a weakness exists when the greatest common 
divisor of both moduli, $n_1$ and $n_2$, is greater than 1. If $GCD(n_1, 
n_2) = p$, then $p$ must be a shared prime factor of $n_1$ and $n_2$. Thus, 
$q_1 = \frac{n_1}{p}$ and $q_2 = \frac{n_2}{p}$. Once $p$ and $q$ are known,
$d_1$ and $d_2$ (values in the private components) can be directly calculated,
yielding both private key pairs. This vulnerability is detailed in
Figure~\ref{fig:vuln}.

This weakness is discussed in \cite{lenstra2012ron}, which shows a significant
number of existing RSA keys were susceptible to this exploit. The primary goal
of our work was to speedup the most computationally intensive part of their
process by implementing the GCD comparisons of RSA 1024-bit keys using NVIDIA's
CUDA platform.

To aid in accomplishing this goal, the work in \cite{fujimoto2009high} was 
expanded and adapted to compare all combinations of keys in a given set. In 
comparison to their work, larger sections of the overall program were able to 
be executed in parallel, resulting in further speedup.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Background}
\label{sec:background}

\section{RSA}
\label{subsec:rsa}
RSA is an asymmetric key encryption scheme. Keys come in matched pairs: a
public key and a private key. The public key is comprised of a modulus $n$ of
specified length (the product of primes $p$ and $q$), and an exponent $e$. The
length of $n$ is given in terms of bits, thus the term ``1024-bit RSA key"
refers to the number of bits which make up this value. The associated private
key uses the same $n$, and another value $d$ such that $d \cdot e = 1
\:\text{mod} \;\phi(n)$ where $\phi(n) = (p - 1) \cdot (q - 1)$
\citep{rivest1978method}.

Using the same algorithm, information encrypted with the one key, can be
decrypted with the other and vice versa. A party must keep the private key
secret, but the public portion of the key can be seen and used by anyone in the
world. To generate a pair, an algorithm is performed using two
randomly-generated prime numbers whose product is of a certain bit-length (e.g.
1024 bits). PARIS aims to uncover the inadequacies present in these primes.

%The work presented by \cite{lenstra2012ron} shows that around 0.2\% of RSA keys
%collected from sources including the SSL Observatory suffer from inadequate
%random prime generation. They show that when the primes used to build an RSA
%public-private key pair are not actually random, factoring does not need to be
%done to break the keys. Instead, only a greater common divisor needs to be
%calculated between two RSA moduli. When anything greater than 1 is found as a
%result of this calculation, the keys can be considered broken and offer no
%protection. This is because once a GCD greater than 1 is found, the private
%keys can be generated from the publicly available information. The
%vulnerability is explained in detail in Figure~\ref{fig:vuln}.

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{vulnerability}
   \caption{Explanation of poor-prime vulnerability}
   \label{fig:vuln}
\end{figure}

%A tool by \cite{scharfglass2012breaking} performs a
%parallelized version of this GCD calculation (using the binary GCD algorithm)
%between every pair combination in a given set of keys. It structures these
%calculations so that as many as possible can be done at once. To increase
%parallelism, and because the keys are so large, they are broken up into 32
%chunks, which are also operated on in parallel using the GPU. Through these 2
%levels of parallelism, a speedup of 27.5 is achieved. This tool provides a
%practical tool for checking a given set of keys for the vulnerability we are
%focusing on.

\section{CUDA}
\label{subsec:cuda}
CUDA is a platform that provides a set of tools along with the ability to 
write programs that make use of NVIDIA's GPUs (cf. 
\cite{nvidia2012programming}). These massively-parallel hardware devices are 
capable of processing large amounts of data simultaneously, allowing 
significant speedups in programs with sections of parallelizable code using 
the Simultaneous Instruction, Multiple Data (SIMD) model. The platform allows
for various arrangements of threads to perform work, based on the developer's
decomposition of the problem. Our solution is discussed in
$\S$\ref{sec:gridorg}. In general, individual threads are grouped into up-to
3-dimensional blocks to allow sharing of common memory between threads. These
blocks can then be organized into a 2-dimensional grid.  

The GPU breaks the total number of threads into groups called warps, which 
consist of 32 threads that will be executed simultaneously on a single
\textit{streaming multiprocessor} (SM). The GPU consists of several SMs which 
are each capable of executing a warp. Blocks are scheduled to SMs until all 
allocated threads have been executed. 

There is also a memory hierarchy on the GPU. There are 3 types of memory 
that are relevant to this work: global memory is the slowest and 
largest; shared memory is much faster, but also significantly smaller; and 
a limited number of registers that each SM has access to. Each thread in a
block can access the same section of shared memory.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Motivation}
\label{sec:motivation}
Despite PARIS's focus on performance and speedup of the vulnerability check,
its impacts and implications to computer security are equally as important.
RSA is widely used and we look into some of its more popular applications to
help motivate why our work and PARIS's increase in practicality is important to
the world of computer security. We aim to discuss what types of data might be
vulnerable due to this exploit (as well as others effecting RSA), impacts the
vulnerability has, and put a portion of the current state of computer security
into a real-world context concerning RSA.

Most initial research yielded general statements that did not explain or give
appropriate context to RSA (statements including ``\dots all over the
Internet\dots" were accurate, but not informative). More in-depth research
yielded two primary use-cases of RSA: Digital Rights Management (DRM) and
Secure Socket Layer (SSL) / Transport Layer Security (TLS) Internet
security protocols. Other uses included password alternatives (e.g. ssh
connections or command line interface tools like \texttt{git}) but these are
more difficult to collect data for and analyze since they tend to be done on
an individual basis, and are managed by individual users.

\section{Digital Rights Management}
\label{subsec:drm}
Digital rights management is a protocol used to secure the usage and
distribution of various types of media content. It is adopted by content
providers and device manufacturers to ensure users don't misuse or wrongfully 
share protected, copyrighted content.
The RSA Association proposed a protocol that could be applied to various types
of media content that secured its usage
\citep{rsa2004announces,rsa2004supports}. These announcements argued that
using RSA for DRM offered benefits to the content providers, the device
manufacturers, and, arguably, the consumers of the content. It apparently
allowed device manufactures and content providers to ensure proper usage of
protected content while allowing users to consume and playback their
rightfully-owned media on an array of their own devices.

\section{Internet Security and Certificates}
\label{subsec:netsec}
Internet security relies on a particular protocol called SSL and its successor
TLS. These protocols define how to securely transfer information over the
Internet by using encryption and signing mechanisms. One aspect of both the SSL
and TLS protocols involve certificates to ensure the expected party is
actually being communicated with. These certificates provide information
about a server that a user is connected to (e.g. Amazon or Google)
and is signed by a certificate authority (e.g. VeriSign). These certificates
are also encrypted with a subject's (e.g. Amazon) public key to ensure that
the entity is who they say they are. Furthermore, the public key is then used 
to transfer information back to the subject in a secure way.
Figure~\ref{fig:tls} describes the TLS process between a client and server. 

The most widely used type of certificate is the X.509 certificate, whose current
revision is 3. A breakdown of the major sections of the certificate is shown in
Figure~\ref{fig:x509}. The ``Subject Public Key Information" section of the
certificate holds the relevant data (the subject's RSA public key) for our work.
The public key portion of the certificates can actually use one of several
algorithms defined in the specification. RSA is by far the most popular,
accounting for over half of the found certificates (as shown in
Figure~\ref{fig:certs}). Other options include DSA and Diffie-Hellman. For more
detailed information about X.509 certificates and analysis on their
infrastructure, see \citet{holz2011ssl}.

\begin{figure}
   \centering
   \includegraphics[width=0.75\textwidth]{x509}
   \caption{X.509 Fields \citep{holz2011ssl}\label{fig:x509}}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[width=0.75\textwidth]{tls}
   \caption{TLS Overview \citep{tlsconcepts}\label{fig:tls}}
\end{figure}

\section{Data Collection}
\label{subsec:datacol}
The proposed DRM protocol given by the RSA actually modeled the certificate
model used by SSL/TLS. For this reason, the SSL certificate model was the
primary focus of the data collected. Moreover, the data set of RSA keys in SSL
certificates is much larger than DRM implementations that use RSA. Thus, the
data set that was collected and then analyzed consisted entirely of X.509
certificates.

Since a primary motivation of this work is the potential impact that vulnerable
RSA keys may have, a data set representative of a large portion of traffic was
desired. A survey was done over 2007-2010 \citep{labovitz2011internet} that
gathered some useful data on the Internet as a whole. Relevant to web traffic,
this study showed that together, the major content data networks (CDN) (i.e.
Google, Facebook, Amazon, etc.) account for nearly 17\% of web traffic.
Specific breakdowns and conversions can be found in Table~\ref{tab:traffic}.

\begin{table}
\centering
\caption{Major CDN Traffic (2010)}
\begin{tabular}{|>{\raggedright}p{0.4\linewidth}
                |>{\raggedright}p{0.2\linewidth}
                |>{\raggedright\arraybackslash}p{0.2\linewidth}|}\hline
   \textbf{CDN} & \textbf{Percent of all Internet traffic} & \textbf{Approx. percentage of web traffic}\\ \hline
Google & 7 & 12.72\\ \hline
Facebook & 0.45& 0.818\\ \hline
Amazon CDN (NASA/JPL, PBS,\dots) & 0.55 & 1\\ \hline
EdgeCast (Yahoo!, Break, Imgur,\dots) & 0.5 & 0.909\\ \hline
LeaseWeb (Heineken, Starbucks,\dots) & 0.8 & 1.454\\
\hline\end{tabular}
\label{tab:traffic}
\end{table}

Alexa was used to find websites with large amounts of
traffic\footnote{http://www.alexa.com/}, as it serves as one of the Internet's
most prevalent sources for traffic information. In addition to individualized
site traffic data, Alexa provides a daily-updated list of the top one million
websites ordered by
traffic\footnote{http://s3.amazonaws.com/alexa-static/top-1m.csv.zip}. In
regards to the previous point concerning CDNs and traffic breakdowns, it was
noted that all web sites hosted by the major CDNs from
\cite{labovitz2011internet} are present in the top one million list.
Additionally, the vast majority of sites from the top one million list were
not provided by the CDNs mentioned in \cite{labovitz2011internet}. This means
that a much larger percentage than 17\% is actually represented by the sites
in the list, although quantifying precisely how much becomes difficult, and
lies beyond the scope of this project.

A Python script was used to parse the CSV file provided by Alexa. Each
extracted URL was visited on port 443 (corresponding to ``https://") using 
\texttt{openssl}. If a site responded on that port, it provided an X.509
certificate to be parsed and verified. During a normal web session, this is
done by a user's web browser which has certificate authorities' private keys
hidden within. However, since the work here was not concerned with the actual
web content, the certificate was simply saved. When certificates were found,
the ``Subject Public Key Algorithm" section was inspected. If this contained
some form of RSA, \texttt{openssl} was used again to extract the RSA key and
save it into a PEM file (a standard format for saving public and private key
information in). Additionally, since part the motivation of data collection is
PARIS, the keys were also stored into a SQLite3 database so that later
retrieval using PARIS was trivial.  

After writing an initial implementation of the Python script, it was realized
that the collection process was too slow. Therefore, some of Python's
multi-process capabilities were utilized to speed up the collection of data.
Eight processes were spawned to perform the work described above. Seven of the
processes read URLs from a deque (double-ended queue) and attempted to obtain
an X.509 certificate and RSA key from each. When found, these keys were added
to a queue. These data structures not only offered convenient structures for
organizing the data, but were also thread-safe and could be used with our
updated implementation. An eighth thread read keys from the queue and entered
them into the SQLite3 database. This reimplementation offered a 15$\times$
speedup over the previous, na\"{i}ve approach.

\section{Data Analysis}
\label{subsec:datanalysis}
After collecting data from each of the top one million Alexa sites, some
surface-level analysis was performed in order to gain an overview of the
security of these one million sites. This analysis included overall security
breakdowns (i.e. whether a site used any kind of security) and
bit-length breakdowns of the RSA keys that were found. This data analysis can
be found in Figures~\ref{fig:certs} and \ref{fig:bits}. Table~\ref{tab:bits}
offer specific breakdown for bit-lengths, and offers details that cannot be
seen in the figure.

\begin{figure}
   \centering
   \includegraphics[width=0.75\linewidth]{cert_security}
   \caption{Presence of RSA key or other security in Alexa top 1M sites}
   \label{fig:certs}
\end{figure}

Interestingly (and slightly worrisome), is that the majority of web sites in
this top one million list did not even respond on port 443, implying that
they do not allow any option for secure traffic to their site. Nearly 60\% of
websites in the list fell into this category.

Also of note are the specifics of bit-lengths found. It is a surprising data
set for various reasons. First, bit-lengths as small as 384, 512, and 768 were
not expected since keys of this size were factored years ago
\citep{rsa2007challenge}, and cannot be expected to offer much
security\footnote{Interesting note: The author attempted to visit the sites
384-bit keys. Google Chrome did not find a web page on port 443 for any of
the sites.}.

On the other hand, the high number of longer bit-lengths was also surprising,
but comforting. The fact that the vast majority of keys were 2048-bit was not
expected, but means that a large number of high-traffic websites have adequate
(for now) encryption. Moreover, there were quite a number of sites with 4096,
and, even more surprisingly, 8192-bit.

\begin{figure}
   \centering
   \includegraphics[width=0.75\linewidth]{bit_length}
   \caption{Breakdown of Alexa top 1M sites' RSA keys by bit-length}
   \label{fig:bits}
\end{figure}

\begin{table}
\centering
\caption{Alexa top 1M X.509 RSA bit-length\label{tab:bits}}
\begin{tabular}{|l|r|}\hline
\textbf{Bits} & \textbf{Count}\\\hline
384 & 4 \\ \hline
512 & 309 \\ \hline
768 & 4 \\ \hline
1024 & 46736 \\ \hline
2048 & 168391 \\ \hline
2432 & 37 \\ \hline
3072 & 17 \\ \hline
4096 & 4511 \\ \hline
8192 & 14 \\ \hline
other & 44 \\ \hline
\end{tabular}
\end{table}

Following the surface-level analysis, PARIS was used to perform a complete
check of all RSA keys in a set in order to check for the previously mentioned
vulnerability \citep{lenstra2012ron}. At the time this data analyzed, PARIS was
limited to 1024-bit RSA keys. Due to this limitation, the data set was reduced
to 46,736 RSA keys. Due to the $O(n^2)$ nature of this problem, this check
resulted in $1,092,150,216$ comparisons. The run of the tool using this data
set took 138 minutes, 37 seconds or $131,305$ comparisons per second.  After
applying the tool to this set of keys, it was found that no keys were
susceptible to the vulnerability (insofar as none of the keys in the set shared
primes with any other key in the set). Although no vulnerabilities were
detected, this does not mean the set is completely secure from the previously
mentioned vulnerability. Since such a limited data set was used, and the
security tool relies on large sets to accumulate many primes, the analysis
presented can only be considered an initial pass.  

\section{Implications}
\label{subsec:implications}
For the same reasons the data set was chosen to be X.509 certificates, presence
of the vulnerability mentioned here would have the largest effect on Internet
security: specifically in the certificate infrastructure. If servers were
unknowingly providing compromised public keys in their certificates, their
traffic could offer no security if this became known. This could offer
ill-effects since the vulnerability is agnostic of the type of data being
being encrypted.

Additionally, the password-alternatives that were mentioned briefly before
could be particularly impacted. If a system was generating many new RSA keys
(e.g. a user is building new accounts for various websites, and tying RSA keys
to them) with inadequate primes, potentially all these accounts for this user
could end up compromised, despite the sense of stronger security.

Along these lines, a system administrator could potentially suffer from similar
issues. If many machines and key-pairs are being setup and generated as part of
an initial setup process for a new network of users, poor random number
generation could have severe consequences when done at scale. This would likely
create many new, vulnerable keys at once. By being aware that this problem
could exist, and that a tool such as PARIS exists, an insecure set of keys 
could be avoided or detected much earlier than would have otherwise been
possible.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related Works}
\label{sec:related}

\section{The Vulnerability}
\label{subsec:vuln}
An RSA vulnerability was discovered and detailed in \cite{lenstra2012ron} in
early 2012 that forms the basis of the exploit the work presented here builds
upon. Namely, that poor random number generation in RSA keys results in
insecure encryption using these keys. Specifically, a key's private components
can be generated using publicly available information.

The exploit explored in \cite{lenstra2012ron} makes use of the two prime
values that play a fundamental role in an RSA key. These values must be
sufficiently random such that repeated values are infeasible and mathematically
extremely improbable to encounter due to the scope of the set (PARIS focuses on
1024-bit keys). However, on some systems and due to some less-than-adequate
code, this may not always be the case. When these values are repeated between
keys, the greatest-common-divisor algorithm can be applied to find the shared
value between keys. This is significant because the GCD calculation is
significantly less computationally expensive than the alternative: factoring
the large values. With this new information about each key, they private
components are straight-forward to generate since the RSA key-generation
process is public and not difficult to implement.  

A large sample set was obtained via the OpenSSL Observatory and by crawling the
Internet for SSL certificates (which use RSA as an encryption scheme). Since it
was thought this would provide an adequate sample set, all these keys were
analyzed. The set contained 1024-bit keys, as well as 2048-bit keys. The
findings were that 0.2\% of these keys were vulnerable to this exploit, and
thus offered no security. This included 2048-bit keys as well, despite the
conception 2048-bit keys are more secure (this would be true if this
vulnerability were avoided).

\section{The Process}
\label{subsec:process}
As mentioned, the vulnerability is exploited by performing greatest common
divisor (GCD) calculations with pairs of keys. The research presented here
looks at how to most quickly and efficiently analyze a large database of
RSA keys and detect if this vulnerability is present within the set.
Traditionally, this would be a tedious process that would likely be infeasible
due to time constraints. Since the calculations are independent of one another
(between pairs of keys) they can be done in parallel, given the proper system.
One such system, and the focus of this work, is NVIDIA's CUDA. This will be
detailed later, however, it relates to work done in \cite{fujimoto2009high}. 

This work optimized a specific version of the GCD (binary GCD). This method
reduces the algorithm to three repeated operations. This algorithm was then
implemented in CUDA for 1024-bit numbers (something CUDA does not offer native
support for). This allows much of each operation to be performed in a parallel
fashion, further optimizing the process.

\section{Multiple GPUs}
\label{subsec:muliGPU}
%As an extension to the work in \cite{scharfglass2012breaking}, multiple GPU
%support will be explored. This concept is discussed at length in
%\cite{schaa2009exploring}. Here, several GPU configurations are investigated to
%analyze how a distributed system may benefit over a single-GPU design.
%Work is divided intelligently among the systems in the distributed system to
%gain more performance increase than would be possible with a single system.
%
%At the time \cite{schaa2009exploring} was written, single-system,
%multiple-GPU configurations were not possible using CUDA over SLI (Scalable
%Link Interface, NVIDIA's brand for the interface between multiple GPUs).
%However, this is possible now, and will be explored at length in our work.
%This configuration offers benefits including transfer speeds over alternative
%configurations.

The work done in \cite{thibault2009cuda} helps to make a case for the
additional speedup that making use of multiple GPUs can offer. This work was
done after CUDA had added adequate support for using multiple GPUs
simultaneously, and configurations with two-GPU and four-GPU systems were used.

Significant additional speedups were gained, ranging up to three times speedup
using four GPUs over the single-GPU system (which translates to 100 times
speedup compared to the original, non-GPU benchmark being used). This kind of
speedup offers great optimism in how multiple GPUs may offer significant
increases to performance of the GCD key algorithm.

Another interesting aspect of the work done in \cite{thibault2009cuda} is the
varied domain sizes that were used. The data sets were broken down into several
different configurations which was each tested on the single, dual, or quad-GPU
configuration. This was done to achieve a more accurate answer to how much
faster the multi-GPU system could be. The largest domain set
($1024\times32\times1024$), interestingly, offered the greatest additional
speedup between configurations---the quad-GPU configuration was 3 times faster
than the single-GPU, compared to 2 times for the $256\times32\times256$
domain. This is significant, and implies that exploring different ways to
organize our own data may offer increased advantages when moving to a multi-GPU
system.

\section{Large Data}
\label{subsec:largedata}
\cite{wu2009clustering} explores a data set, like our own, where the data is
too large to fit within the extremely-limited memory of the GPU. Thus, the data
must be partitioned. In this work, the data was first prepared by organizing it
so that it could be effectively partitioned. There was a straightforward way of
doing so for this application (K-means).  

Additionally, some features of the GPU were considered and used, namely
asynchronous memory transfer, a type of multi-tasked streaming. This feature
allows memory transfers to be done between the GPU and CPU while the GPU is
processing data it already has. This can increase performance significantly
when compared to having the GPU move from processing to data transfer (i.e.
having a single tasked thread). Optimization of threads was looked into in this
work as well, as the authors attempted to find how many of these threads would
add the positively impact speedup the most. They found that only 2 threads
(i.e. one processing, one transferring) were needed to gain the greatest
performance benefit. Adding additional streams after this point did not offer
additional benefit.

%Decomposition of data is the primary problem when attempting to perform work
%with a GPU due to the limitations of current hardware. Therefore, much work and
%thought has been given to optimizing the structure of large data sets. Although
%a different type of parallelism is looked at in \cite{charles2012chunking}, the
%strategies to data decomposition are relevant to most parallel problem in
%general, including CUDA. The focus in this survey is matrix multiplication,
%a standard parallel problem. The most effective way to chunk the data in this
%work appears to be entire rows/columns of the element matrices. This is logical,
%and the approach is mimicked (as much as it can be since matrix multiplication
%is not perfectly synonymous) in \cite{scharfglass2012breaking}. However, a slight
%modification was found to make this strategy most effective: hold the results
%in memory until an entire set of iterations completes (all that will fit in
%memory) and only then transfer the results back to the CPU. Although this does
%not decrease the amount of memory sent from the GPU to the CPU, it reduces the
%number of total memory transfers which is often more important. Such fine
%details must be found and applied to our work here to further increase our
%performance.

\section{Optimization}
\label{subsec:opti}
As mentioned, data decomposition and organization around the architecture are
some of the most important (if not \emph{the} most important) aspects of
creating a parallel solution for use with a GPGPU. The work done in
\cite{ryoo2008optimization} gives a detailed overview of how one might optimize
work done with CUDA. It discusses the memory model, and how certain techniques
such as tiling are used to optimize the performance gains.

Unfortunately, this work was done before multi-GPU systems were fully available
and accessible using CUDA. The optimization strategies presented in this work
will no doubt be useful; however, other strategies also must be employed that
considered systems with more than a single GPU (such as those mentioned here
from \cite{thibault2009cuda}). Leveraging techniques in both realms will offer
the greatest amount of performance gain.

\section{Similar Algorithms}
\label{subsec:simialg}
A similar algorithm that suffers from the same problems that pairwise
comparisons does (each element much be traversed and compared), is search. The
work done in \cite{peters2011fast} explores how search can be optimized for the
CUDA framework. Two approaches are given to the bitonic search algorithm that
is used: the initial, most straight-forward implementation, and another
implementation optimized around the CUDA memory model. The latter offered
significant performance increases over the original. 

%A similar approach will be applied to the work that is used from
%\cite{scharfglass2012breaking}. Namely, shared memory will be used as efficiently
%as possible to allow CUDA to perform at its greatest capacity. This is a vital
%concept in general when optimizing for CUDA; however, the work and challenges
%from \cite{peters2011fast} are similar to our own, so the implementation
%overlaps more than arbitrary examples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Algorithm}
\label{sec:alg}

\section{Binary GCD}
\label{subsec:binGCD}
Binary GCD is a well known algorithm for computing the greatest common divisor 
of two numbers. Instead of relying on costly division operations like Euclid's 
algorithm, bit-wise shifts are employed. The implementation
presented in this paper follows the outline displayed in
Algorithm~\ref{alg:binGCD}.

\begin{algorithm}
   \KwIn{$x$ and $y$: two integers.}
   \KwOut{The greatest common divisor of $x$ and $y$.}
   \nl\Repeat {$GCD(x, y) = GCD(0, y) = y$} {
      \nl\uIf {$x$ and $y$ are both even} {
         \nl$GCD(x, y) = 2 \cdot GCD(\frac{x}{2}, \frac{y}{2})$\;
      }\nl\uElseIf{$x$ is even and $y$ is odd} {
         \nl$GCD(x, y) = GCD(\frac{x}{2}, y)$\;
      }\nl\uElseIf {$x$ is odd and $y$ is even} {
         \nl$GCD(x, y) = GCD(\frac{y}{2}, x)$\;
      }\nl\ElseIf {$x$ and $y$ are both odd} {
         \nl\eIf {$x \geq y$} {
            \nl$GCD(x, y) = GCD(\frac{x - y}{2}, y)$\;
         }{
            \nl$GCD(x, y) = GCD(\frac{y - x}{2}, x)$\;
        }
      }
   }
   \caption{Binary GCD algorithm outline}
   \label{alg:binGCD}
\end{algorithm}

\section{Parallel Functions}
\label{subsec:parfunc}
To accomplish Algorithm~\ref{alg:binGCD} using CUDA, the following three 
functions had to parallelized: shift, subtract, and greater-than-or-equal. As 
outlined in \cite{fujimoto2009high}, each 1024-bit number is divided across one 
warp so that each thread has its own 32-bit integer. 

The parallel shift function is straightforward: each thread is given an 
equal-sized piece of the large-precision integer. Then each thread except for 
Thread 0 grabs a copy of the integer at \texttt{threadID  - 1}.
The variable \texttt{threadID} refers to a value between 0 and 31 and corresponds 
to a thread in a warp. Each thread shifts its value once and 
uses its copy of the adjacent integer to determine if a bit has shifted 
between threads. This procedure is outlined in Algorithm~\ref{alg:parShift}.
\begin{algorithm}
   \KwIn{$x[32]$ is a 1024-bit integer represented as an array of 32 \texttt{int}s, $threadID$ is the 0-31 index of the thread in warp.}
   \nl\eIf {$threadID \neq 0$} {
      \nl$temp\gets x[threadID - 1]$\;
   }{
      \nl$temp\gets 0$\;
   }
   \nl$x\gets x>>1$\;
   \nl$x\gets x \:\text{OR}\: (temp << 31)$\;
   \caption{Parallel right shift}
   \label{alg:parShift}
\end{algorithm}

The parallel subtract uses a method called \emph{carry skip} from 
\cite{fujimoto2009high}. First, each thread subtracts its piece and sets the 
``borrow" flag of \texttt{threadID - 1} if an underflow occurred. Next, each 
thread checks if it was borrowed from and if so, decrements itself and clears the 
flag. Then, if another underflow occurs, the borrow flag at
\texttt{threadID - 1} will be set. This continues until all the borrow flags
are cleared. An outline can be found in Algorithm~\ref{alg:parSub}.

\begin{algorithm}
   \KwIn{$x$ and $y$: two 1024-bit integers, $threadID$ is the 0-31 index of the thread in warp.}
   \nl$x[threadID]\gets x[threadID]-y[threadID]$\;
   \nl\If {underflow} {
      \nl set $borrow[threadID - 1]$\;
   }
   \nl\Repeat {all $borrow$ flags are cleared} {
   \nl\If {$borrow[threadID]$ is set} {
      \nl$x[threadID]\gets x[threadID] - 1$\;
         \nl\If {underflow} {
            \nl set $borrow[threadID - 1]$\;
         }
         \nl clear $borrow[threadID]$\;
      }
   }
   \caption{Parallel subtract using ``carry skip"}
   \label{alg:parSub}
\end{algorithm}

The parallel greater-than-or-equal has each thread check if its integers are 
equal. If this is the case, then it sets a position variable shared by the warp 
to the minimum of its \texttt{threadID} and the current value stored in the position variable.
This is done atomically to ensure the correct value is stored. Finally, all 
the threads do a greater-than-or-equal comparison with the integers specified
by the position variable. This function is outlined in Algorithm~\ref{alg:parGEQ}.
\begin{algorithm}
   \KwIn{$x$ and $y$: two 1024-bit integers, $threadID$ is the 0-31 index of the thread in warp.}
   \KwOut{$True$ if $x \geq y$; else $False$.}
   \nl\If {$x[threadID] \neq y[threadID]$} {
      \nl$pos\gets \text{atomicMin}(threadID, pos)$\;
   }
   \nl\Return $x[pos] \geq y[pos]$
   \caption{Parallel greater-than-or-equal-to}
   \label{alg:parGEQ}
\end{algorithm}

\section{Computational Complexity}
\label{subsec:compcomp}
The computational complexity of the binary GCD algorithm has been shown by 
Stein and Vall$\acute{\text{e}}$e (cf. \cite{stein1967computational}, 
\cite{vallee1998complete}) to have a worst case complexity of $\mathcal{O}(n^2)$ 
where $n$ is the number of bits in the integer. The worst case is produced 
when each iteration of the algorithm shifts one of its arguments only once. 
Since for this application $n$ is fixed at 1024 bits, the complexity of a 
single GCD calculation can be considered to be constant time for the worst case.

To compare all the keys together, the amount of GCDs that must be calculated 
grows at a rate of $k^2$, where $k$ is the number of keys.

\section{Theoretical Speedup}
\label{subsec:theory}
Maximum speedup is defined in Equation~\ref{eq:speed}:
\begin{equation}
   \mbox{Max Speedup} = \frac{1}{1 - P}
   \label{eq:speed}
\end{equation}

where $P$ is the percentage of the program's execution that can be parallelized.
This percentage is a function of the number of keys the program needs to 
process, and is calculated in Equation~\ref{eq:percent}.
\begin{equation}
P = \frac{t \cdot g}{t \cdot g + r \cdot k}
   \label{eq:percent}
\end{equation}
where
\begin{itemize}
   \item $t$ = time to process a single GCD
   \item $g$ = total number of GCD calculations
   \item $r$ = time to read a single key
   \item $k$ = total number of keys
\end{itemize}

Since $g$ will increase significantly more rapidly than $k$, $P$ (based on 
Equation~\ref{eq:percent}) will approach $1$ as $k$ approaches 
$\infty$. This relationship can be observed in Figure~\ref{fig:parPercent}.

\begin{figure}
   \centering
   \includegraphics[width=0.75\textwidth]{chart_7}
   \caption{Total percentage of CUDA implementation that is parallel}
   \label{fig:parPercent}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Problem Description}
\label{sec:probdesc}
The RSA weakness described above demands that each key in a set be 
compared with each other key to determine if a GCD greater than 1 exists for 
any pair. Given a known set of keys, it is not known before processing 
the keys which will be likely to have a GCD greater than 1; therefore, there 
is no way to eliminate comparisons between specific pairs. The natural 
organization to fulfill this requirement is a comparison matrix of the all 
keys. Each location in the matrix corresponds to a GCD comparison between two 
keys.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation}
\label{sec:impl}

\section{Problem Decomposition}
\label{subsec:probdecomp}

Initially, the comparison matrix seems to be an $n^2$ solution. However, the 
diagonal of the matrix created consists of unproductive GCD calculations since 
these entries would compare each key with itself. Furthermore, the matrix is 
symmetrical over the diagonal. Thus, only the comparisons comprising one of 
the triangles needs to be performed. Specifically, 
\begin{equation}
   \mbox{Total number of GCD compares} = \sum_{i=1}^k i
   \label{eq:gcd}
\end{equation}

%This reduction in number of overall compares decreases the work 
%performed significantly, shown in Figure~\ref{fig:compvkeys}. 

%\begin{figure}
   %\centering
   %\includegraphics[width=0.75\textwidth]{chart_6}
   %\caption{Number of comparisons needed vs. total number of keys in set}
   %\label{fig:compvkeys}
%\end{figure}

\section{Grid Organization}
\label{sec:gridorg}
One of the most important aspects of any CUDA implementation is the 
organization of the thread and block array to ensure that the architecture is 
appropriately used to its full potential. The threads array in this 
implementation was organized using 3 dimensions. The $x$ dimension represented 
the sectioning of a 1024-bit value into individual 32-bit integers, of which 
there are 32. 
\begin{displaymath}
   \frac{1024 \:\mbox{bits per key}}{32 \:\mbox{bits per integer}} = 
   32 \;\mbox{integers per key}
\end{displaymath}
The remaining dimensions, $y$ and $z$, were set to 4, resulting in a block of 
512 threads. This design decision was experimentally determined. See 
$\S$\ref{sec:occupancy} for details about Occupancy optimizations.
\begin{displaymath}
   32 \cdot 4 \times 4 = 512
\end{displaymath}
This ensured that each block remained square for algorithmic 
symmetry and simplicity. The $y$ and $z$ dimensions corresponded to how many 
specific keys within the list of all keys were being compared per block. 
Thus, two 1024-bit keys were loaded into each 32-thread warp, which was 
then processed simultaneously as a single comparison. The $x$ dimension was 
chosen for two reasons: 1) so one thread in this dimension would represent 
each of the 32-bit integers inside the key and 2) because there are 32 threads 
in a single warp. Therefore, this thread-array organization ensured that 
compares were done using two entire keys (separated into 32 pieces) that were 
scheduled to the same warp. This eliminated warp divergence since every warp 
was filled and executed with non-overlapping data.

Blocks were arranged in row-major order based on the key comparisons that they 
held. The formula for the number of blocks, $B$, needed for a vector of keys of 
size $k$ can be seen in Equation~\ref{eq:blocks}.
\begin{equation}
   \sum_{i=1}^{\left\lceil \frac{k}{4} \right\rceil}i = B
   \label{eq:blocks}
\end{equation}

The limit for a grid in a single dimension is $2^{16} - 1 = 65535$ 
and limits the amount of keys that can be processed to $1444$. To increase 
the number of blocks available for computation, a second grid dimension was 
added. This increased the theoretical maximum number of keys per kernel 
launch as seen in Equation~\ref{eq:maxKeys}.
\begin{equation}
   \begin{split}
   \sum_{i = 1}^{\left\lceil\frac{k}{4}\right\rceil} i & \leq {\left(2^{16} - 
   1\right)}^2\\
   k & \leq 370716\\
   \end{split}
   \label{eq:maxKeys}
\end{equation}

\section{Shared Memory}
\label{subsec:shared}
Shared memory was used to load the necessary keys from global memory. Two 
arrays were created in shared memory, representing the thread-array; both 
3-dimensional, $32\times4\times4$ and had an integer loaded into each 
available space. Each array represented which integers would be compared at 
each location in the matrix. A side effect of this organization was that each key would be 
repeated 4 times within its integer array. However, this greatly simplified 
the GCD algorithm so that only a look-up into each array was needed. Since 
shared memory was not the limiting factor for occupancy, it was not a priority 
to optimize this aspect of the design and implementation. 

Shared memory was also used within the GCD algorithm, specifically in the 
greater-than-or-equal-to function, and the subtract function. In the 
greater-than-or-equal-to function, a single integer was allocated for each 
comparison within a block. Within the subtract function, shared memory was 
utilized to represent the borrow value for each integer. 

\section{Occupancy}
\label{sec:occupancy}
Each SM can be assigned multiple blocks at the same time as long as there are 
enough free registers and shared memory available. The ratio of active warps 
to the maximum number of warps supported by a SM is called \emph{occupancy}. 
On the Fermi architecture, the maximum occupancy is achieved when there are 48 
active warps running on a SM at one time. Greater occupancy gives a SM more
opportunities to schedule warps in a fashion to hide memory accesses, thus,
saturating a SM with many warps decreases performance impact. CUDA Fermi cards
have a total of 32768 registers and 49152 bytes of shared memory per SM. The
implementation here uses 17 registers and 4762 bytes of shared memory per
block and therefore results in a maximum occupancy of 100\%.

By using the CUDA occupancy calculator provided by NVIDIA (cf. 
\cite{nvidia2012gpu}), a plot can be formed comparing the threads per block 
with occupancy. To maintain the same block organization outlined above, the 
block dimensions can be $2\times2, 3\times3, 4\times4, 5\times5$ or 128, 228, 
512, 800 threads, respectively. Table~\ref{tab:occupancy} shows the calculated 
occupancy for these block sizes. A block size of 512 threads was chosen 
because it results in the greatest occupancy and thus the best performance. 

\begin{table}
   \centering
   \begin{tabular}{l|rrrr}
      Threads per block & 128 & 288 & 512 & 800\\
      \hline
              Occupancy & 67\% & 94\% & 100\% & 52\%\\
   \end{tabular}
   \caption{Table giving occupancy for various block dimensions}
   \label{tab:occupancy}
\end{table}

\section{Bit-vector}
\label{subsec:bitvector}
The initial approach was to allocate a large, multi-dimensional array of 
integers that would hold the results of the CUDA GCD calculations. This was 
allocated to the GPU, so each thread could have access as needed; however, 
since the number of results grew at $n^2$, the lack of scalability in this 
approach was quickly apparent. Additionally, performance decreased due to the 
large array that was being sent over the PCIe bus. Memory transfers to 
the GPU are slow, and must be minimized.

After more careful consideration, a new approach was implemented. There would 
only be a single bit allocated per key-compare to mark whether or not the pair 
had a GCD greater than 1. In this way, only 2 bytes (16 bits = 1 bit per 
compare) were necessary per block ($4\times4 = 16$ compares per block), as 
opposed to the previous $16 \cdot 32 \cdot 4 = 2048$ bytes. Despite not having access 
to the answer immediately after returning from the kernel calculation, this 
approach would be more efficient since there would be a theoretically small 
number of keys that actually returned with GCDs greater than 1 (i.e. the flag 
was set). This small set could then be re-processed (GCDs calculated) using a 
different kernel or using a CPU algorithm. Efficiency would also be increased 
due to the time saved in memory transfers since there was significantly less 
memory to transfer before calling the kernel.

\subsection{Grid Organization}
\label{sec:gridorg}
One of the most important aspects of any CUDA implementation is the 
organization of the thread and block array to ensure that the architecture is 
appropriately used to its full potential. The threads array in this 
implementation was organized using 3 dimensions. The $x$ dimension represented 
the sectioning of a 1024-bit value into individual 32-bit integers, of which 
there are 32. 
\begin{displaymath}
   \frac{1024 \:\mbox{bits per key}}{32 \:\mbox{bits per integer}} = 
   32 \;\mbox{integers per key}
\end{displaymath}
The remaining dimensions, $y$ and $z$, were set to 4, resulting in a block of 
512 threads. This design decision was experimentally determined. See 
$\S$\ref{sec:occupancy} for details about Occupancy optimizations.
\begin{displaymath}
   32 \cdot 4 \times 4 = 512
\end{displaymath}
This ensured that each block remained square for algorithmic 
symmetry and simplicity. The $y$ and $z$ dimensions corresponded to how many 
specific keys within the list of all keys were being compared per block. 
Thus, two 1024-bit keys were loaded into each 32-thread warp, which was 
then processed simultaneously as a single comparison. The $x$ dimension was 
chosen for two reasons: 1) so one thread in this dimension would represent 
each of the 32-bit integers inside the key and 2) because there are 32 threads 
in a single warp. Therefore, this thread-array organization ensured that 
compares were done using two entire keys (separated into 32 pieces) that were 
scheduled to the same warp. This eliminated warp divergence since every warp 
was filled and executed with non-overlapping data.

Blocks were arranged in row-major order based on the key comparisons that they 
held. The formula for the number of blocks, $B$, needed for a vector of keys of 
size $k$ can be seen in Equation~\ref{eq:blocks}.
\begin{equation}
   \sum_{i=1}^{\left\lceil \frac{k}{4} \right\rceil}i = B
   \label{eq:blocks}
\end{equation}

The limit for a grid in a single dimension is $2^{16} - 1 = 65535$ 
and limits the amount of keys that can be processed to $1444$. To increase 
the number of blocks available for computation, a second grid dimension was 
added. This increased the theoretical maximum number of keys per kernel 
launch as seen in Equation~\ref{eq:maxKeys}.
\begin{equation}
   \begin{split}
   \sum_{i = 1}^{\left\lceil\frac{k}{4}\right\rceil} i & \leq {\left(2^{16} - 
   1\right)}^2\\
   k & \leq 370716\\
   \end{split}
   \label{eq:maxKeys}
\end{equation}

\subsection{XY Coordinate mapping}
\label{subsubsec:xymap}

Due to the work-reduction step taken in Section~\ref{subsec:probdecomp}, a
traditional CUDA 2D block arrangement was not appropriate. The block indexes
that would have been advantageous, would have wasted significant resources on
the GPU since we only aim to do half of the work that a full 2D block grid would
provide. Therefore, the block array that was used was essentially one
dimensional. Two dimensions are used (as described in
Section~\ref{subsec:gridorg}) when there are many keys that cannot be allocated
to the GPU with a single CUDA block dimension. However, this second dimension
only adds quantity to the one dimensional list, but the indexes advantages
normally realized by the CUDA block grid could not be used.

To overcome this, an XY coordinate mapping was precomputed and copied to the
GPU. This mapping was a lookup table that took sequential block numberings from
the upper triangular comparison matrix, and converted them to x-y coordinate
pairs that would have been accurate in the full comparison matrix. This was a
necessary step due to indexing into the key sets during each comparison and to
look up vulnerable keys once the process is complete.

\subsection{Shared Memory}
\label{subsec:shared}
Shared memory was used to load the necessary keys from global memory. Two 
arrays were created in shared memory, representing the thread-array; both 
3-dimensional, $32\times4\times4$ and had an integer loaded into each 
available space. Each array represented which integers would be compared at 
each location in the matrix. A side effect of this organization was that each key would be 
repeated 4 times within its integer array. However, this greatly simplified 
the GCD algorithm so that only a look-up into each array was needed. Since 
shared memory was not the limiting factor for occupancy, it was not a priority 
to optimize this aspect of the design and implementation. 

Shared memory was also used within the GCD algorithm, specifically in the 
greater-than-or-equal-to function, and the subtract function. In the 
greater-than-or-equal-to function, a single integer was allocated for each 
comparison within a block. Within the subtract function, shared memory was 
utilized to represent the borrow value for each integer. 

\subsection{Occupancy}
\label{sec:occupancy}
Each SM can be assigned multiple blocks at the same time as long as there are 
enough free registers and shared memory available. The ratio of active warps 
to the maximum number of warps supported by a SM is called \emph{occupancy}. 
On the Fermi architecture, the maximum occupancy is achieved when there are 48 
active warps running on a SM at one time. Greater occupancy gives a SM more
opportunities to schedule warps in a fashion to hide memory accesses, thus,
saturating a SM with many warps decreases performance impact. CUDA Fermi cards
have a total of 32768 registers and 49152 bytes of shared memory per SM. The
implementation here uses 17 registers and 4762 bytes of shared memory per
block and therefore results in a maximum occupancy of 100\%.

By using the CUDA occupancy calculator provided by NVIDIA (cf. 
\cite{nvidia2012gpu}), a plot can be formed comparing the threads per block 
with occupancy. To maintain the same block organization outlined above, the 
block dimensions can be $2\times2, 3\times3, 4\times4, 5\times5$ or 128, 228, 
512, 800 threads, respectively. Table~\ref{tab:occupancy} shows the calculated 
occupancy for these block sizes. A block size of 512 threads was chosen 
because it results in the greatest occupancy and thus the best performance. 

\begin{table}
   \centering
   \begin{tabular}{l|rrrr}
      Threads per block & 128 & 288 & 512 & 800\\
      \hline
              Occupancy & 67\% & 94\% & 100\% & 52\%\\
   \end{tabular}
   \caption{Table giving occupancy for various block dimensions}
   \label{tab:occupancy}
\end{table}

\subsection{Bit-vector}
\label{subsec:bitvector}
The initial approach was to allocate a large, multi-dimensional array of 
integers that would hold the results of the CUDA GCD calculations. This was 
allocated to the GPU, so each thread could have access as needed; however, 
since the number of results grew at $n^2$, the lack of scalability in this 
approach was quickly apparent. Additionally, performance decreased due to the 
large array that was being sent over the PCIe bus. Memory transfers to 
the GPU are slow, and must be minimized.

After more careful consideration, a new approach was implemented. There would 
only be a single bit allocated per key-compare to mark whether or not the pair 
had a GCD greater than 1. In this way, only 2 bytes (16 bits = 1 bit per 
compare) were necessary per block ($4\times4 = 16$ compares per block), as 
opposed to the previous $16 \cdot 32 \cdot 4 = 2048$ bytes. Despite not having access 
to the answer immediately after returning from the kernel calculation, this 
approach would be more efficient since there would be a theoretically small 
number of keys that actually returned with GCDs greater than 1 (i.e. the flag 
was set). This small set could then be re-processed (GCDs calculated) using a 
different kernel or using a CPU algorithm. Efficiency would also be increased 
due to the time saved in memory transfers since there was significantly less 
memory to transfer before calling the kernel.

\subsection{Arbitrary Length Key Sets}
\label{subsec:arblength}
PARIS's initial implementation \citep{scharfglass2012breaking} took into
account all the above implementation details; however, it was only able to run
a single CUDA kernel once.. Furthermore, comparison runs were constrained by
available memory on a given GPU, so only runs that would fit into the limited
memory could be run successfully.

Decomposing the set of comparisons for an arbitrary number of keys is difficult
due to the $n^2$ nature of the problem. The above-outlined method cannot simply
be repeated for various segments of a key set since this would cause gaps in
the results because comparisons between the segments will be missing. Thus, the
entire comparison matrix was segmented, rather than the key sets, into
manageable sections that were run sequentially.

The full comparison matrix still exists as described above: the upper triangle
of a square matrix where each entry is a GCD comparison of two unique keys from
a provided set. This upper triangle is divided into smaller sections of two
types: smaller upper triangles that lie on the diagonal, and rectangles that
reside in the upper portion of the original matrix (see
Figure~\ref{fig:divkeys1}). This partitioning mechanism was chosen for several
reasons, the primary of which is that each of these divisions results in nearly
identical memory requirements and workloads that are passed to the GPU. 

\begin{figure}
   \centering
   \includegraphics[width=0.7\textwidth]{one_division}
   \caption{A single division of the key matrix}
   \label{fig:divkeys1}
\end{figure}

Depending on the size of the key set provided, it is possible that one of the
divisions shown in Figure~\ref{fig:divkeys1} would still not fit into the GPU
memory. In these cases, a further division is done in a similar way. The upper
triangular segments are partitioned identically as was shown in
Figure~\ref{fig:divkeys1}, and the rectangles are also divided into quadrants. 
Figure~\ref{fig:divkeys3} shows the segments after two additional division
steps. This division process continues until a single segment can fit into the
GPU memory.  

\begin{figure}
   \centering
   \includegraphics[width=0.7\textwidth]{three_divisions}
   \caption{Three divisions of the key matrix}
   \label{fig:divkeys3}
\end{figure}

To determine when to end the division process, an upper bound for the maximal
number of keys that can fit onto the GPU must be determined. First, we evaluate
what the memory requirements of each segment so we can maximize. We'll use the
following values in our memory calculations:

\begin{displaymath}
   \begin{split}
   T = & \mbox{ the memory needed by an upper triangular segment}\\
       & \mbox{ of key comparisons}\\
   R = & \mbox{ the memory required by a rectangular segment}\\
       & \mbox{ of key comparisons}\\
   y = & \mbox{ number of keys on the y axis being compared}\\
   x = & \mbox{ number of keys on the x axis being compared}\\
   B = & \mbox{ number of CUDA blocks necessary for this key set}\\
   KEY\_SIZE = & \mbox{ the size (in bytes) of a key modulus (128)}\\
   XY\_SIZE = & \mbox{ the size (in bytes) of the x-y coordinate (4)}\\
   GCD\_SIZE = & \mbox{ the size (in bytes) necessary to hold GCD result}\\
               & \mbox{ bits for a single block (2)}
   \end{split}
\end{displaymath}

The triangular portions along the diagonal require 3 allocations: the set of
keys to compare, the bit-vector to hold the GCD result, and an x-y coordinate
mapping to aid the look-up of keys within the provided set. The x-y coordinate
pair consists of two 16-bit integers (one for $x$, one for $y$), thus the 4
bytes total. The memory calculation is shown in Equation~\ref{eq:trimem}.  

\begin{equation}
   T = y \cdot KEY\_SIZE + B \cdot XY\_SIZE + B \cdot GCD\_SIZE
   \label{eq:trimem}
\end{equation}

\noindent Recall,
\begin{equation}
   B = \sum_{i=1}^{\left\lceil \frac{k}{4} \right\rceil}i
   \tag{\ref{eq:blocks}}
\end{equation}
where $k = $number of keys. We will substitute $k$ for $y$ in the following
calculations since they refer to the same value.  

\noindent Thus,
\begin{equation}
   B = \sum_{i=1}^{\left\lceil \frac{y}{4} \right\rceil}i
   \label{eq:blocksy}
\end{equation}

\noindent We will use the arithmetic sum identity
(Equation~\ref{eq:arithident}; proof in Appendix~\ref{app:proofs}) to make the
substitution in Equation~\ref{eq:blocksub}.  

\begin{equation}
   \sum_{i=1}^{n}i = \frac{1}{2}n\left(n+1\right)
   \label{eq:arithident}
\end{equation}

\begin{equation}
   B = \frac{1}{2}\left(\frac{y}{4}\right) \left(\frac{y}{4}+1\right)
   \label{eq:blocksub}
\end{equation}

\noindent Continuing Equation~\ref{eq:trimem},
\begin{align}
   T & = y \cdot KEY\_SIZE + B \cdot (XY\_SIZE + GCD\_SIZE)\notag\\
   T & = y \cdot 128 + B \cdot (4 + 2)\notag\\
   T & = y \cdot 128 + \left(\frac{1}{2}\left(\frac{y}{4}\right)\left(\frac{y}{4}+1\right)\right) \cdot 6\notag\\
   T & = 128 y + 3 \cdot \frac{y^2}{16} + 3 \cdot \frac{y}{4}\notag\\
   T & = 3 \cdot \frac{y^2}{16} + \frac{515y}{4}
   \label{eq:trimemcomplete}
\end{align}

The rectangular portions also require 3 allocations: the set of keys in the $y$
direction, the set of keys in the $x$ direction, and the bit-vector to hold the
GCD result. Equation~\ref{eq:rectmem} outlines the calculation of $R$. Due to
the difference in shape of these GCD results, the number of blocks will not use
the same formula. Instead, the number of blocks is just the area of the
rectangle, shown in Equation~\ref{eq:rectblocks}. Recall, the number of keys is
divided by the dimension of the blocks described in $\S$\ref{sec:gridorg}.

\begin{equation}
   B' = \frac{y}{4} \cdot \frac{x}{4}
   \label{eq:rectblocks}
\end{equation}

\begin{align}
   R & = y \cdot KEY\_SIZE + x \cdot KEY\_SIZE + B' \cdot GCD\_SIZE\\
   R & = (y + x) \cdot KEY\_SIZE + B' \cdot GCD\_SIZE\notag\\
   R & = (y + x) \cdot 128 + \left(\frac{y}{4} \cdot \frac{x}{4}\right) \cdot 2\notag\\
   \intertext{As mentioned (and as can be seen in Figure~\ref{fig:divkeys1}), $x$ will be half of $y$. Thus,}
   R & = (y + \frac{y}{2}) \cdot 128 + \left(\frac{y}{4} \cdot \frac{\frac{y}{2}}{4}\right) \cdot 2\notag\\
   R & = 128y + 64y + \left(\frac{y}{4} \cdot \frac{y}{4}\right)\notag\\
   R & = \frac{y^2}{16} + 192y
   \label{eq:rectmem}
\end{align}

Comparing $T$ and $R$, we can see that for key sets larger than 506 keys, $T$ is larger. The difference between the functions is increasing, so $T$ will remain the larger value. Therefore, we can use $T$ as the upper bound for memory requirements. 

With an equation for evaluating the maximum amount of memory required for a specified number of keys (Equation~\ref{eq:trimemcomplete}), if a limit on memory exists, we can determine the maximal number of keys that can fit within that memory. CUDA offers a function to query the total amount of memory a device has available (we'll call this $F$). With this value, Equation~\ref{eq:keycalc} can be used to find the number of keys, $y$.

\begin{align*}
   T & \leq F\\
   3 \cdot \frac{y^2}{16} + \frac{515y}{4} & \leq F\\
   3 \cdot \frac{y^2}{16} + \frac{515y}{4} - F & \leq 0\\
   3 \cdot y^2 + 4\cdot515y- 16 \cdot F & \leq 0\\
   y & = \frac{-b \pm \sqrt{b^2 - 4 \cdot a \cdot c}}{2a}\\
   \text{ with } & a = 3, b = 2060, c = -16F
\end{align*}
\noindent we disregard the `-' since we can't a negative number of keys
\begin{equation}
   y = \frac{1}{6}\left(-2060 + \sqrt{2060^2 - 192 \cdot F}\right)
   \label{eq:keycalc}
\end{equation}

Equation~\ref{eq:keycalc} is used to find a maximum number of keys for a GPU. The complete comparison matrix is then segmented as described above until the size of a segment is smaller than this maximal $y$. The matrix is then iterated over until all results are computed.

\subsection{Generating the Private Key}
\label{subsec:private}
% TODO
Once the entire set has been processed and the GPU work is complete, the bit-vector is inspected for ``1"s signifying a vulnerable key was found. When a ``1" is found, the GCD needs to be calculated to generate the private key. As was shown in Figure~\ref{fig:vuln}, the result of the GCD will be one of the original prime values. Each of the two moduli that was used in the GCD calculation is then divided by this value to calculate the other corresponding primes. The totient ($\phi$) is calculated using the primes. Finally $d$ is the result of finding the modular multiplicative inverse of $\phi$ and $e$ (also contained in the public key).

\subsection{Multiple GPUs}
\label{subsec:multiGPU}
%TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Experimental Setup}
\label{sec:expsetup}

\section{Test Machine}
\label{subsec:testmachine}

\subsection{Initial Setup}
\label{subsubsec:initsetup}
All performance measurements were made on a single machine with an Intel Xeon 
W3503 dual-core CPU and 4 GB of RAM. This machine has one NVIDIA GeForce
GTX 480 GPU with 480 CUDA cores and 1.5 GB of memory. The CUDA driver 
version present on the machine is 4.2.0, release 302.17, the runtime version is
4.2.9. The CUDA compute capability is version 2.0, and the maximum threads per 
block is 1024, with each warp having 32 threads.

\subsection{Updated Setup}
\label{subsubsec:updatedsetup}
% TODO
Kepler!

\section{Reference Implementations}
\label{subsec:refimpl}
In order to check the accuracy of the final implementation, as well as to 
provide a point of comparison for benchmarking, two reference implementations 
of this exploit were created. Each was able to use the same format key 
databases (described in $\S$\ref{subsec:testsets}). 

The first implementation was written purely in Python using the open source
PyCrypto cryptography library. This implementation was able to perform the
entire exploit, from finding weak 1024-bit RSA public keys through generating
the discovered private keys. This implementation was not used for performance
comparison as it was dissimilar to the other two implementations. However, it
was used to validate the exploit itself and serve as an algorithm reference.

A sequential version of the binary GCD algorithm was implemented to serve as a 
second validation tool for the CUDA implementation. This version sequentially 
processed the same input as both other implementations and produced output of
the same format. Comparison with this implementation ensured that unexpected
errors did not result merely from processing the data in parallel.

\section{Test Sets}
\label{subsec:testsets}
In order to conduct meaningful tests, it was necessary to use an identical 
data set in all tests. To facilitate this, a tool was written in Python to 
generate both regular and intentionally weak RSA key pairs using PyCrypto and
store them in an SQLite3 database. All keys were generated with a 
constant $e$ of 65537, chosen because this was discovered to be a commonly used 
value (cf. \cite{lenstra2012ron}).

The generation process produced a database of RSA key pairs. 
Intentionally weak keys were evenly distributed. 

In order to generate a weak key, this program would generate an initial 
normal RSA key but save the prime used for $p$. For each subsequent bad key, 
$p$ would be replaced with this constant, and $n$ was recalculated. The 
result was that each weak key would have a GCD greater than 1 when 
tested against any other weak key, namely, $p$. 

Using this tool, it was possible to build arbitrarily large test sets with a 
known number of keys exhibiting the weakness. When these databases 
were processed using any of the reference implementations, the 
discovered number of weak keys could be directly compared with the 
number of keys expected to be found. This allowed both repeatable testing to 
measure run time, and a method to validate the parallel algorithm was indeed 
finding GCDs as expected. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Results}
\label{sec:results}

\section{Initial Implementation}
\label{subsec:initimpl}
The accuracy of the parallel implementation was verified against the 
sequential implementation by using identical test data sets with known 
weak keys. Since both implementations found the same set of compromised keys,
it was validated that these two implementations 
were internally consistent. Furthermore, both matched the results of the 
separate Python reference implementation: supporting the assertion of accurate 
functionality. The speedup of the CUDA implementation (seen in
Figure~\ref{fig:speedup}) was calculated by comparing its run time with that
of the sequential implementation. Compare this with Figure~\ref{fig:parPercent}:
this similarity is evidence of the implementation presented here matching
with theoretical expectations.

\begin{figure}
   \centering
   \includegraphics[width=0.75\textwidth]{chart_1}
   \caption{Speedup of CUDA Implementation to Sequential C++}
   \label{fig:speedup}
\end{figure}

Figure~\ref{fig:speedup} shows that speedup increases dramatically with 
the number of keys until about 2000 comparisons. At this point, the GPU
becomes saturated with enough blocks to fully occupy all of the SMs. Speedup
remains constant at 27.5 for up to 200000 keys. We have no data beyond this
number of keys due to the very long run time of the sequential implementation. 

\begin{table*}
   \centering
   \begin{tabular}{|l|*{5}{r}|}\hline
      Number of Keys        & 20   & 200  & 2000   & 20000    & 200000 \\ \hline
      Sequential Time (sec) & 0.13 & 5.59 & 550.69 & 55121.86 & 185551.91\\
      CUDA Time (sec)       & 0.21 & 0.31 & 20.21  & 2005.09  & 6748.23\\\hline
      \textbf{Speedup}      & 0.6  & 18.0 & 27.2   & 27.5     & 27.5\\\hline
   \end{tabular}
   \caption{Run-times of sequential and CUDA implementations}
   \label{tab:runtimes}
\end{table*}

\section{Scalable Implementation}
\label{scaleimpl}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}
\label{sec:concl}
A large speedup resulted directly from writing a CUDA 
implementation when compared to the sequential implementation. Many 
more keys are able to be compared in a given amount of time using the CUDA
implementation. 

PARIS is a tool that allows for efficient and complete comparisons of a list of
1024-bit RSA public keys, avoiding repetition and unnecessary work. This tool
allows an increased number of keys to be compared to prior work, in turn
allowing overall execution time to decrease due to the increased parallelism.

PARIS offers significant advantages over other GCD algorithms in CUDA, and
practically applies this for comparison of 1024-bit RSA keys in order to test
for a particular weakness.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Future Work}
\label{sec:future}
To further enhance the throughput of PARIS, asynchronous memory transfers could
be added to the implementation. When combined with multiple kernel launches, a
significant portion of the memory I/O (which is one of the main limiting
factors of performance using the GPU) would be able to be masked by
simultaneously processing the data currently on the GPU while new data is being
copied onto it.

The Kepler architecture introduces new features that may increase the 
performance of this implementation. A feature known as Dynamic Parallelism 
allows a CUDA kernel to launch new kernels from the GPU. This would allow
dynamic allocation of block sizes for different areas of the comparison matrix
and remove idle threads from the kernel. Hyper-Q is a new technology that
manages multiple CUDA kernels from multiple CPU threads. With the current Fermi
architecture, only one CUDA kernel may run on the device at one time. This can
lead to under utilization of the GPU hardware. An approach using multiple CPU
threads, each running their own CUDA kernel, could greatly increase throughput.

\chapter{Appendix}
\appendix
\label{app:proofs}

\begin{proof}[Arithmetic Sum Identity]
\label{proof:asi}
   \begin{align*}
     \text{Let } S & = \sum_{i=1}^{n}i\\
      & = 1 + 2 + \dots + (n - 1) + n\\
      & = n + (n - 1) + \dots + 2 + 1 && \text{Commutative Property}\\
      S + S & = (1 + n) + (2 + (n - 1)) + \dots + ((n - 1) + 2) + (n + 1)\\
      2S & = (n + 1) + (n + 1) + \dots + (n + 1) + (n + 1)\\
      2S & = n \cdot (n + 1) && \text{$n$ terms in the sum}\\
      S & = \frac{1}{2} n (n + 1)\\
      \sum_{i=1}^{n}i & = \frac{1}{2} n (n + 1)
   \end{align*}
\end{proof}
% ------------- End main chapters ----------------------

\clearpage
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}
